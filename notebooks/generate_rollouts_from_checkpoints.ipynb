{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbax.checkpoint as ocp\n",
    "from track_mjx.agent.custom_ppo import TrainingState\n",
    "from track_mjx.agent import custom_losses as ppo_losses\n",
    "from track_mjx.agent import custom_ppo, custom_ppo_networks\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "from brax import envs\n",
    "from brax.training.acme import running_statistics, specs\n",
    "from track_mjx.environment import custom_wrappers\n",
    "\n",
    "from track_mjx.environment.task.multi_clip_tracking import MultiClipTracking\n",
    "from track_mjx.environment.task.single_clip_tracking import SingleClipTracking\n",
    "from track_mjx.environment.task.reward import RewardConfig\n",
    "\n",
    "from track_mjx.environment.walker.rodent import Rodent\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import OmegaConf\n",
    "import pickle\n",
    "import functools\n",
    "\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"./../track_mjx/config\")\n",
    "\n",
    "# Load the config file\n",
    "cfg = compose(config_name=\"rodent-full-clips\")\n",
    "\n",
    "# register the environment\n",
    "envs.register_environment(\"rodent_single_clip\", SingleClipTracking)\n",
    "envs.register_environment(\"rodent_multi_clip\", MultiClipTracking)\n",
    "envs.register_environment(\"fly_multi_clip\", MultiClipTracking)\n",
    "\n",
    "env_cfg = OmegaConf.to_container(cfg, resolve=True)\n",
    "env_args = cfg.env_config[\"env_args\"]\n",
    "env_rewards = cfg.env_config[\"reward_weights\"]\n",
    "train_config = cfg.train_setup[\"train_config\"]\n",
    "walker_config = cfg[\"walker_config\"]\n",
    "traj_config = cfg[\"reference_config\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `TrainingState`\n",
    "\n",
    "Currently I just directly copy and paste the scripts from the training code. In the future for the sake of runtime and cleaner code, we can have a helper function that construct the abstracted `TrainingState` directly without creating the actual environment. This is on Scott's TODOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(Scott): move this to track_mjx.io module\n",
    "input_data_path = \"/root/vast/scott-yang/track-mjx/data/ReferenceClip.p\"\n",
    "with open(input_data_path, \"rb\") as file:\n",
    "    reference_clip = pickle.load(file)\n",
    "\n",
    "walker_map = {\n",
    "    \"rodent\": Rodent,\n",
    "}\n",
    "\n",
    "walker_class = walker_map[env_cfg[\"walker_type\"]]\n",
    "walker = walker_class(**walker_config)\n",
    "\n",
    "# didn't use args** since penalty_pos_distance_scale need conversion\n",
    "reward_config = RewardConfig(\n",
    "    too_far_dist=env_rewards.too_far_dist,\n",
    "    bad_pose_dist=env_rewards.bad_pose_dist,\n",
    "    bad_quat_dist=env_rewards.bad_quat_dist,\n",
    "    ctrl_cost_weight=env_rewards.ctrl_cost_weight,\n",
    "    ctrl_diff_cost_weight=env_rewards.ctrl_diff_cost_weight,\n",
    "    pos_reward_weight=env_rewards.pos_reward_weight,\n",
    "    quat_reward_weight=env_rewards.quat_reward_weight,\n",
    "    joint_reward_weight=env_rewards.joint_reward_weight,\n",
    "    angvel_reward_weight=env_rewards.angvel_reward_weight,\n",
    "    bodypos_reward_weight=env_rewards.bodypos_reward_weight,\n",
    "    endeff_reward_weight=env_rewards.endeff_reward_weight,\n",
    "    healthy_z_range=env_rewards.healthy_z_range,\n",
    "    pos_reward_exp_scale=env_rewards.pos_reward_exp_scale,\n",
    "    quat_reward_exp_scale=env_rewards.quat_reward_exp_scale,\n",
    "    joint_reward_exp_scale=env_rewards.joint_reward_exp_scale,\n",
    "    angvel_reward_exp_scale=env_rewards.angvel_reward_exp_scale,\n",
    "    bodypos_reward_exp_scale=env_rewards.bodypos_reward_exp_scale,\n",
    "    endeff_reward_exp_scale=env_rewards.endeff_reward_exp_scale,\n",
    "    penalty_pos_distance_scale=jnp.array(env_rewards.penalty_pos_distance_scale),\n",
    ")\n",
    "\n",
    "# Automatically match dict keys and func needs\n",
    "environment = envs.get_environment(\n",
    "    env_name=cfg.env_config.env_name,\n",
    "    reference_clip=reference_clip,\n",
    "    walker=walker,\n",
    "    reward_config=reward_config,\n",
    "    **env_args,\n",
    "    **traj_config,\n",
    ")\n",
    "# Episode length is equal to (clip length - random init range - traj length) * steps per cur frame.\n",
    "# Will work on not hardcoding these values later\n",
    "episode_length = (\n",
    "    traj_config.clip_length - traj_config.random_init_range - traj_config.traj_length\n",
    ") * environment._steps_for_cur_frame\n",
    "print(f\"episode_length {episode_length}\")\n",
    "\n",
    "network_factory = functools.partial(\n",
    "    custom_ppo_networks.make_intention_ppo_networks,\n",
    "    encoder_hidden_layer_sizes=tuple(cfg.network_config.encoder_layer_sizes),\n",
    "    decoder_hidden_layer_sizes=tuple(cfg.network_config.decoder_layer_sizes),\n",
    "    value_hidden_layer_sizes=tuple(cfg.network_config.critic_layer_sizes),\n",
    ")\n",
    "\n",
    "seed = 42\n",
    "\n",
    "key = jax.random.PRNGKey(seed)\n",
    "global_key, local_key = jax.random.split(key)\n",
    "local_key, key_env, eval_key = jax.random.split(local_key, 3)\n",
    "# key_networks should be global, so that networks are initialized the same\n",
    "# way for different processes.\n",
    "key_policy, key_value, policy_params_fn_key = jax.random.split(global_key, 3)\n",
    "\n",
    "v_randomization_fn = None\n",
    "\n",
    "if isinstance(environment, envs.Env):\n",
    "    wrap_for_training = custom_wrappers.wrap\n",
    "else:\n",
    "    wrap_for_training = custom_wrappers.wrap\n",
    "\n",
    "train_env = wrap_for_training(\n",
    "    environment,\n",
    "    episode_length=episode_length,\n",
    "    action_repeat=1,\n",
    "    randomization_fn=v_randomization_fn,\n",
    ")\n",
    "\n",
    "reset_fn = train_env.reset\n",
    "key_envs = jax.random.split(key_env, 1)\n",
    "env_state = reset_fn(key_envs)\n",
    "\n",
    "normalize = lambda x, y: x\n",
    "if True:\n",
    "    normalize = running_statistics.normalize\n",
    "ppo_network = network_factory(\n",
    "    env_state.obs.shape[-1],\n",
    "    int(env_state.info[\"reference_obs_size\"][0]),\n",
    "    train_env.action_size,\n",
    "    preprocess_observations_fn=normalize,\n",
    ")\n",
    "make_policy = custom_ppo_networks.make_inference_fn(ppo_network)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=1e-4)\n",
    "\n",
    "init_params = ppo_losses.PPONetworkParams(\n",
    "    policy=ppo_network.policy_network.init(key_policy),\n",
    "    value=ppo_network.value_network.init(key_value),\n",
    ")\n",
    "\n",
    "training_state = TrainingState(  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
    "    optimizer_state=optimizer.init(init_params),  # pytype: disable=wrong-arg-types  # numpy-scalars\n",
    "    params=init_params,\n",
    "    normalizer_params=running_statistics.init_state(specs.Array(env_state.obs.shape[-1:], jnp.dtype(\"float32\"))),\n",
    "    env_steps=0,\n",
    ")\n",
    "\n",
    "abstract_policy = (training_state.normalizer_params, training_state.params.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orbax Checkpoint Related\n",
    "\n",
    "The following code will create the checkpoint manager and restore the checkpoint for both the policy and train state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore the whole model (both policy module, and training state)\n",
    "options = ocp.CheckpointManagerOptions(step_prefix=\"PPONetwork\")\n",
    "with ocp.CheckpointManager(\n",
    "    \"/root/vast/scott-yang/track-mjx/model_checkpoints/rodent_data/ReferenceClip.p_250116_073049/\",\n",
    "    options=options,\n",
    ") as mngr:\n",
    "    print(f\"latest checkpoint step: {mngr.latest_step()}\")\n",
    "    policy = mngr.restore(\n",
    "        mngr.latest_step(), args=ocp.args.Composite(policy=ocp.args.StandardRestore(abstract_policy))\n",
    "    )[\"policy\"]\n",
    "    train_state = mngr.restore(\n",
    "        mngr.latest_step(), args=ocp.args.Composite(train_state=ocp.args.StandardRestore(training_state))\n",
    "    )[\"train_state\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Internal Activations During Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rollout_key = jax.random.PRNGKey(42)\n",
    "ref_trak_config = cfg[\"reference_config\"]\n",
    "\n",
    "env_config = cfg[\"env_config\"]\n",
    "walker_config = cfg[\"walker_config\"]\n",
    "\n",
    "# Wrap the env in the brax autoreset and episode wrappers\n",
    "rollout_env = custom_wrappers.RenderRolloutWrapperTracking(environment)\n",
    "\n",
    "# define the jit reset/step functions\n",
    "jit_reset = jax.jit(rollout_env.reset)\n",
    "jit_step = jax.jit(rollout_env.step)\n",
    "\n",
    "jit_inference_fn = jax.jit(make_policy(policy, deterministic=True, get_activation=True))\n",
    "rollout_key, reset_rng, act_rng = jax.random.split(rollout_key, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state_to_save(state):\n",
    "    state_keys = [\"obs\", \"reward\", \"done\", \"metrics\", \"info\"]\n",
    "    ps_keys = [\n",
    "        \"act\",\n",
    "        \"act_dot\",\n",
    "        \"actuator_force\",\n",
    "        \"actuator_length\",\n",
    "        \"actuator_moment\",\n",
    "        \"actuator_velocity\",\n",
    "        \"bvh_aabb_dyn\",\n",
    "        \"bvh_active\",\n",
    "        \"cacc\",\n",
    "        \"cam_xmat\",\n",
    "        \"cam_xpos\",\n",
    "        \"cdof\",\n",
    "        \"cdof_dot\",\n",
    "        \"cfrc_ext\",\n",
    "        \"cfrc_int\",\n",
    "        \"cinert\",\n",
    "        \"crb\",\n",
    "        \"ctrl\",\n",
    "        \"cvel\",\n",
    "        \"efc_D\",\n",
    "        \"efc_J\",\n",
    "        \"efc_aref\",\n",
    "        \"efc_force\",\n",
    "        \"efc_frictionloss\",\n",
    "        \"efc_margin\",\n",
    "        \"efc_pos\",\n",
    "        \"efc_type\",\n",
    "        \"eq_active\",\n",
    "        \"flexedge_J\",\n",
    "        \"flexedge_J_colind\",\n",
    "        \"flexedge_J_rowadr\",\n",
    "        \"flexedge_J_rownnz\",\n",
    "        \"flexedge_length\",\n",
    "        \"flexedge_velocity\",\n",
    "        \"flexelem_aabb\",\n",
    "        \"flexvert_xpos\",\n",
    "        \"geom_xmat\",\n",
    "        \"geom_xpos\",\n",
    "        \"light_xdir\",\n",
    "        \"light_xpos\",\n",
    "        \"mocap_pos\",\n",
    "        \"mocap_quat\",\n",
    "        \"ncon\",\n",
    "        \"ne\",\n",
    "        \"nefc\",\n",
    "        \"nf\",\n",
    "        \"nl\",\n",
    "        \"q\",\n",
    "        \"qDeriv\",\n",
    "        \"qH\",\n",
    "        \"qHDiagInv\",\n",
    "        \"qLD\",\n",
    "        \"qLDiagInv\",\n",
    "        \"qLDiagSqrtInv\",\n",
    "        \"qLU\",\n",
    "        \"qM\",\n",
    "        \"qacc\",\n",
    "        \"qacc_smooth\",\n",
    "        \"qacc_warmstart\",\n",
    "        \"qd\",\n",
    "        \"qfrc_actuator\",\n",
    "        \"qfrc_applied\",\n",
    "        \"qfrc_bias\",\n",
    "        \"qfrc_constraint\",\n",
    "        \"qfrc_damper\",\n",
    "        \"qfrc_fluid\",\n",
    "        \"qfrc_gravcomp\",\n",
    "        \"qfrc_inverse\",\n",
    "        \"qfrc_passive\",\n",
    "        \"qfrc_smooth\",\n",
    "        \"qfrc_spring\",\n",
    "        \"qpos\",\n",
    "        \"qvel\",\n",
    "        \"sensordata\",\n",
    "        \"site_xmat\",\n",
    "        \"site_xpos\",\n",
    "        \"solver_niter\",\n",
    "        \"subtree_angmom\",\n",
    "        \"subtree_com\",\n",
    "        \"subtree_linvel\",\n",
    "        \"ten_J\",\n",
    "        \"ten_J_colind\",\n",
    "        \"ten_J_rowadr\",\n",
    "        \"ten_J_rownnz\",\n",
    "        \"ten_length\",\n",
    "        \"ten_velocity\",\n",
    "        \"ten_wrapadr\",\n",
    "        \"ten_wrapnum\",\n",
    "        \"time\",\n",
    "        \"userdata\",\n",
    "        \"wrap_obj\",\n",
    "        \"wrap_xpos\",\n",
    "        \"xanchor\",\n",
    "        \"xaxis\",\n",
    "        \"xfrc_applied\",\n",
    "        \"ximat\",\n",
    "        \"xipos\",\n",
    "        \"xmat\",\n",
    "        \"xpos\",\n",
    "        \"xquat\",\n",
    "    ]\n",
    "    pipeline_state = state.pipeline_state\n",
    "    output = {}\n",
    "    for key in state_keys:\n",
    "        output[key] = getattr(state, key)\n",
    "    for key in ps_keys:\n",
    "        output[key] = getattr(pipeline_state, key)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "def generate_rollout(clip_idx: int | None = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate a rollout for a given clip id, with the loaded checkpoint\n",
    "\n",
    "    Args:\n",
    "        clip_id (int): The clip id to generate the rollout for, if None, will generate a random clip\n",
    "\n",
    "    returns:\n",
    "        ctrls (List): The controls for the rollout\n",
    "        extrases (List): The extra outputs from the policy for the rollout\n",
    "        rewards (Dict): The rewards for the rollout\n",
    "    \"\"\"\n",
    "\n",
    "    rollout_key = jax.random.PRNGKey(42)\n",
    "    rollout_key, reset_rng, act_rng = jax.random.split(rollout_key, 3)\n",
    "    # do a rollout on the saved model\n",
    "    state = jit_reset(reset_rng, clip_idx=clip_idx)\n",
    "\n",
    "    rollout_states = [state]\n",
    "    ctrls, activations, rewards = [], [], {}\n",
    "    for i in range(\n",
    "        int(ref_trak_config.clip_length * environment._steps_for_cur_frame) - 1\n",
    "    ):  # why is this? what's the observation for the last few step?\n",
    "        _, act_rng = jax.random.split(act_rng)\n",
    "        obs = state.obs\n",
    "        ctrl, extras = jit_inference_fn(obs, act_rng)\n",
    "        state = jit_step(state, ctrl)\n",
    "        rollout_states.append(state)\n",
    "        ctrls.append(ctrl)\n",
    "        activations.append(extras[\"activations\"])\n",
    "\n",
    "    # might include those reward term in the visual rendering\n",
    "    pos_rewards = [state.metrics[\"pos_reward\"] for state in rollout_states]\n",
    "    endeff_rewards = [state.metrics[\"endeff_reward\"] for state in rollout_states]\n",
    "    quat_rewards = [state.metrics[\"quat_reward\"] for state in rollout_states]\n",
    "    angvel_rewards = [state.metrics[\"angvel_reward\"] for state in rollout_states]\n",
    "    bodypos_rewards = [state.metrics[\"bodypos_reward\"] for state in rollout_states]\n",
    "    joint_rewards = [state.metrics[\"joint_reward\"] for state in rollout_states]\n",
    "    summed_pos_distances = [state.info[\"summed_pos_distance\"] for state in rollout_states]\n",
    "    joint_distances = [state.info[\"joint_distance\"] for state in rollout_states]\n",
    "    torso_heights = [state.pipeline_state.xpos[environment.walker._torso_idx][2] for state in rollout_states]\n",
    "    rewards = {\n",
    "        \"pos_rewards\": pos_rewards,\n",
    "        \"endeff_rewards\": endeff_rewards,\n",
    "        \"quat_rewards\": quat_rewards,\n",
    "        \"angvel_rewards\": angvel_rewards,\n",
    "        \"bodypos_rewards\": bodypos_rewards,\n",
    "        \"joint_rewards\": joint_rewards,\n",
    "        \"summed_pos_distances\": summed_pos_distances,\n",
    "        \"joint_distances\": joint_distances,\n",
    "        \"torso_heights\": torso_heights,\n",
    "    }\n",
    "    # get qposes for both rollout and reference\n",
    "    ref_traj = rollout_env._get_reference_clip(rollout_states[0].info)\n",
    "    qposes_ref = np.repeat(\n",
    "        np.hstack([ref_traj.position, ref_traj.quaternion, ref_traj.joints]),\n",
    "        environment._steps_for_cur_frame,\n",
    "        axis=0,\n",
    "    )\n",
    "    qposes_rollout = np.array([state.pipeline_state.qpos for state in rollout_states])\n",
    "    # processed_states = [process_state_to_save(state) for state in rollout_states]\n",
    "    observations = [state.obs for state in rollout_states]\n",
    "    output = {\n",
    "        \"rewards\": rewards,\n",
    "        \"observations\": observations,\n",
    "        # \"states\": processed_states,\n",
    "        \"ctrl\": ctrls,\n",
    "        \"activations\": activations,\n",
    "        \"qposes_ref\": qposes_ref,\n",
    "        \"qposes_rollout\": qposes_rollout,\n",
    "        \"info\": [state.info for state in rollout_states],\n",
    "    }\n",
    "\n",
    "    output = jax.tree.map(lambda x: np.array(x), output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to HDF5 File\n",
    "\n",
    "Since we have a pytree dictionary data structure, we might need to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "output = generate_rollout(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to save out the observations, info, metrics, done, reward, in the state, and the mujoco variable in the pipeline state if we want to do analysis. Let's save a complete list of item.\n",
    "\n",
    "Also, need to serialize the clip information to make the video rendering directly reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "run the rollout for all of the clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def save_to_h5py(file, data, group_path=\"/\"):\n",
    "    \"\"\"\n",
    "    Save a pytree (like a dictionary) into an HDF5 file.\n",
    "\n",
    "    Args:\n",
    "        file (h5py.File): An open HDF5 file object.\n",
    "        data: The data to save (can be a dictionary, list, etc.).\n",
    "        group_path (str): The HDF5 group path for saving the data.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            sub_group_path = f\"{group_path}/{key}\"\n",
    "            save_to_h5py(file, value, sub_group_path)\n",
    "    elif isinstance(data, list):\n",
    "        for i, item in enumerate(data):\n",
    "            sub_group_path = f\"{group_path}/{i}\"\n",
    "            save_to_h5py(file, item, sub_group_path)\n",
    "    elif isinstance(data, (int, float, str, bool, np.ndarray)):\n",
    "        file.create_dataset(group_path, data=data)\n",
    "    elif hasattr(data, \"numpy\"):  # For NumPy arrays or PyTorch tensors\n",
    "        file.create_dataset(group_path, data=data.numpy())\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type: {type(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for i in tqdm(range(environment._n_clips)):\n",
    "    output = generate_rollout(i)\n",
    "    with h5py.File(f\"/root/vast/scott-yang/rodent_rollout_info/clip_{i}.h5\", \"w\") as h5file:\n",
    "        save_to_h5py(h5file, output)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_h5py(file, group_path=\"/\"):\n",
    "    \"\"\"\n",
    "    Load a pytree structure from an HDF5 file.\n",
    "\n",
    "    Args:\n",
    "        file (h5py.File): An open HDF5 file object.\n",
    "        group_path (str): The HDF5 group path to read data from.\n",
    "\n",
    "    Returns:\n",
    "        The reconstructed data structure.\n",
    "    \"\"\"\n",
    "    group = file[group_path]\n",
    "    if isinstance(group, h5py.Dataset):\n",
    "        return group[()]  # Read dataset value\n",
    "    elif isinstance(group, h5py.Group):\n",
    "        if all(k.isdigit() for k in group.keys()):  # Likely a list\n",
    "            return [load_from_h5py(file, f\"{group_path}/{k}\") for k in sorted(group.keys(), key=int)]\n",
    "        else:  # Dictionary-like group\n",
    "            return {k: load_from_h5py(file, f\"{group_path}/{k}\") for k in group.keys()}\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported group type: {type(group)}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "with h5py.File(\"clip1_rollout.h5\", \"r\") as h5file:\n",
    "    loaded_data = load_from_h5py(h5file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_info = {}\n",
    "for i in tqdm(range(environment._n_clips)[:50]):\n",
    "    crtls, extrases, rewards = generate_rollout(i)\n",
    "    rollout_info[i] = {\"ctrls\": crtls, \"extrases\": extrases, \"rewards\": rewards}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "intentions, clips = [], []\n",
    "for i in range(50):\n",
    "    rollout = rollout_info[i][\"extrases\"]\n",
    "    for j in range(int(episode_length)):\n",
    "        intention = rollout[j][\"activations\"][\"intention\"]\n",
    "        intentions.append(intention)\n",
    "        clips.append(i)\n",
    "intentions, clips = np.array(intentions), np.array(clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pca = PCA()\n",
    "# tsne = TSNE(n_components=2, random_state=42, n_jobs=-1, perplexity=30)\n",
    "\n",
    "# embedded = tsne.fit_transform(intentions, clips)\n",
    "pca = pca.fit(intentions)\n",
    "print(np.cumsum(pca.explained_variance_ratio_[:10]))\n",
    "pca_embedded = pca.transform(intentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_embedded[:, 0], pca_embedded[:, 1], c=clips, cmap=\"tab20\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_ind = 0\n",
    "\n",
    "is_clip = clips == clip_ind\n",
    "clip_intentions = intentions[is_clip]\n",
    "clip_projections = pca.transform(clip_intentions)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for pc_ind in range(4):\n",
    "    plt.plot(clip_projections[:, pc_ind], label=f\"PC {pc_ind} ({pca.explained_variance_ratio_[pc_ind]*100:.1f}%)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time (steps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering\n",
    "\n",
    "Video on the left, and the PCA of the intention rolling out.\n",
    "\n",
    "Walker type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_ind = 0\n",
    "\n",
    "is_clip = clips == clip_ind\n",
    "clip_intentions = intentions[is_clip]\n",
    "clip_projections = pca.transform(clip_intentions)\n",
    "\n",
    "clip_df = pd.DataFrame(clip_projections[:, :10], columns=[f\"PC {i+1}\" for i in range(10)])\n",
    "clip_df[\"time\"] = np.arange(clip_projections.shape[0])\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.lineplot(data=clip_df, x=\"PC 1\", y=\"PC 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(embedded[:, 0], embedded[:, 1], c=clips, cmap=\"tab20\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rollout save\n",
    "\n",
    "`xarray` `hdf5`. Everything including states, obs. Dataset (datapoint session) should be modular.\n",
    "\n",
    "save file hdf5, one each clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track_mjx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
