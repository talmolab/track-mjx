"""
PPO implementation using flax new API, nnx.

Proximal policy optimization training.

See: https://arxiv.org/pdf/1707.06347.pdf
"""

# Copyright 2024 The Brax Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Proximal policy optimization training.

See: https://arxiv.org/pdf/1707.06347.pdf
"""

import functools
import time
from typing import Callable, Optional, Tuple, Union, Any

from absl import logging
from brax import base
from brax import envs
from track_mjx.brax_nnx import gradients
from track_mjx.brax_nnx import acting
from brax.training import pmap
from brax.training import types
from brax.training.acme import running_statistics
from brax.training.acme import specs

# from brax.training.agents.ppo import losses as ppo_losses
# from track_mjx.agent import custom_losses as ppo_losses
from track_mjx.agent import nnx_ppo_losses as ppo_losses
from track_mjx.agent.nnx_ppo_network import (
    make_intention_ppo_networks,
    PPOImitationNetworks,
    PPOTrainConfig,
)


# from brax.training.agents.ppo import networks as ppo_networks
from track_mjx.agent import nnx_ppo_network
from brax.training.types import Params
from brax.training.types import PRNGKey
from brax.v1 import envs as envs_v1
import flax
from flax import nnx
from flax.training import train_state
import jax
import jax.numpy as jnp
import numpy as np
import optax

from track_mjx.environment import custom_wrappers


InferenceParams = Tuple[running_statistics.NestedMeanStd, Params]
Metrics = types.Metrics

_PMAP_AXIS_NAME = "i"


class TrainingState(train_state.TrainState):
    """Contains training state for the learner."""

    # optimizer_state: optax.OptState # this is opt_state
    normalizer_params: running_statistics.RunningStatisticsState
    env_steps: jnp.ndarray
    other_variables: nnx.State


def _unpmap(v):
    return jax.tree_util.tree_map(lambda x: x[0], v)


def _strip_weak_type(tree):
    # brax user code is sometimes ambiguous about weak_type.  in order to
    # avoid extra jit recompilations we strip all weak types from user input
    def f(leaf):
        leaf = jnp.asarray(leaf)
        # print(str(leaf.dtype))
        if str(leaf.dtype) == "key<fry>":
            leaf = leaf
        else:
            leaf = leaf.astype(jnp.float32)
        return leaf

    return jax.tree_util.tree_map(f, tree)


def train(
    environment: envs.Env,
    config: PPOTrainConfig,
):
    """PPO training.

    Args:
      environment: the environment to train
      num_timesteps: the total number of environment steps to use during training
      episode_length: the length of an environment episode
      action_repeat: the number of timesteps to repeat an action
      num_envs: the number of parallel environments to use for rollouts
        NOTE: `num_envs` must be divisible by the total number of chips since each
          chip gets `num_envs // total_number_of_chips` environments to roll out
        NOTE: `batch_size * num_minibatches` must be divisible by `num_envs` since
          data generated by `num_envs` parallel envs gets used for gradient
          updates over `num_minibatches` of data, where each minibatch has a
          leading dimension of `batch_size`
      max_devices_per_host: maximum number of chips to use per host process
      num_eval_envs: the number of envs to use for evluation. Each env will run 1
        episode, and all envs run in parallel during eval.
      learning_rate: learning rate for ppo loss
      entropy_cost: entropy reward for ppo loss, higher values increase entropy
        of the policy
      discounting: discounting rate
      seed: random seed
      unroll_length: the number of timesteps to unroll in each environment. The
        PPO loss is computed over `unroll_length` timesteps
      batch_size: the batch size for each minibatch SGD step
      num_minibatches: the number of times to run the SGD step, each with a
        different minibatch with leading dimension of `batch_size`
      num_updates_per_batch: the number of times to run the gradient update over
        all minibatches before doing a new environment rollout
      num_evals: the number of evals to run during the entire training run.
        Increasing the number of evals increases total training time
      num_resets_per_eval: the number of environment resets to run between each
        eval. The environment resets occur on the host
      normalize_observations: whether to normalize observations
      reward_scaling: float scaling for reward
      clipping_epsilon: clipping epsilon for PPO loss
      gae_lambda: General advantage estimation lambda
      deterministic_eval: whether to run the eval with a deterministic policy
      network_factory: function that generates networks for policy and value
        functions
      progress_fn: a user-defined callback function for reporting/plotting metrics
      normalize_advantage: whether to normalize advantage estimate
      eval_env: an optional environment for eval only, defaults to `environment`
      policy_params_fn: a user-defined callback function that can be used for
        saving policy checkpoints
      randomization_fn: a user-defined callback function that generates randomized
        environments

    Returns:
      Tuple of (make_policy function, network params, metrics)
    """
    # make sure the batch size is divisible by the number of envs
    assert config.batch_size * config.num_minibatches % config.num_envs == 0, (
        config.batch_size * config.num_minibatches % config.num_envs
    )
    xt = time.time()

    # Get the number of devices and set up multiple devices supports
    process_count = jax.process_count()
    process_id = jax.process_index()
    local_device_count = jax.local_device_count()
    local_devices_to_use = local_device_count
    if config.max_devices_per_host:
        local_devices_to_use = min(local_devices_to_use, config.max_devices_per_host)
    logging.info(
        "Device count: %d, process count: %d (id %d), local device count: %d, " "devices to be used count: %d",
        jax.device_count(),
        process_count,
        process_id,
        local_device_count,
        local_devices_to_use,
    )
    device_count = local_devices_to_use * process_count

    # The number of environment steps executed for every training step.
    env_step_per_training_step = (
        config.batch_size * config.unroll_length * config.action_repeat # take out the minibatch * config.num_minibatches
    ) # Scott: This metrics for the environment performance is not true -- minibatch and permutation is bootstrapping
    # data instead of directly 
    num_evals_after_init = max(config.num_evals - 1, 1)

    # The number of training_step calls per training_epoch call.
    # equals to ceil(num_timesteps / (num_evals * env_step_per_training_step *
    #                                 num_resets_per_eval))
    num_training_steps_per_epoch = np.ceil(
        config.num_timesteps / (num_evals_after_init * env_step_per_training_step * max(config.num_resets_per_eval, 1))
    ).astype(int)

    # initialize random keys for the MJX environment
    key = jax.random.PRNGKey(config.seed)
    global_key, local_key = jax.random.split(key)
    local_key = jax.random.fold_in(local_key, process_id)
    local_key, key_env, eval_key = jax.random.split(local_key, 3)
    assert config.num_envs % device_count == 0

    v_randomization_fn = None

    if isinstance(environment, envs.Env):
        wrap_for_training = custom_wrappers.wrap
    else:
        raise ValueError(" Brax V1 envs are deprecated, please use envs.Env")

    env = wrap_for_training(
        environment,
        episode_length=config.episode_length,
        action_repeat=config.action_repeat,
        randomization_fn=v_randomization_fn,
    )

    # create parallelized environment with dimensions from random number keys
    reset_fn = jax.jit(jax.vmap(env.reset))
    key_envs = jax.random.split(key_env, config.num_envs // process_count)
    key_envs = jnp.reshape(key_envs, (local_devices_to_use, -1) + key_envs.shape[1:])
    env_state = reset_fn(key_envs)
    # need to remove batch dim if not doing pmap
    env_state = jax.tree_util.tree_map(lambda x: jnp.squeeze(x, axis=0), env_state)

    normalize = lambda x, y: x
    if config.normalize_observations:
        normalize = running_statistics.normalize

    # define PPO network, now it is a nnx module.
    ppo_network = config.network_factory(
        env_state.obs.shape[-1],
        int(_unpmap(env_state.info["reference_obs_size"])),
        env.action_size,
        preprocess_observations_fn=normalize,
    )

    # define optimizer
    optimizer = nnx.Optimizer(ppo_network, optax.adam(learning_rate=config.learning_rate))

    # create loss function
    loss_fn = functools.partial(
        ppo_losses.compute_ppo_loss,
        entropy_cost=config.entropy_cost,
        kl_weight=config.kl_weight,
        discounting=config.discounting,
        reward_scaling=config.reward_scaling,
        gae_lambda=config.gae_lambda,
        clipping_epsilon=config.clipping_epsilon,
        normalize_advantage=config.normalize_advantage,
    )

    num_minibatches = config.num_minibatches

    def gradient_update_fn(model, data, loss_fn, optimizer, key):
        grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)
        (total_loss, details), grads = grad_fn(model, data, key)
        optimizer.update(grads)
        return total_loss, details

    # vmap only for the data with the 0 axis batch dim
    def minibatch_step(
        model: nnx.Module,
        optimizer: nnx.Optimizer,
        data: types.Transition,
        key: PRNGKey,
        # normalizer_params: running_statistics.RunningStatisticsState, # TODO: implement obs normalizer
    ):
        print("DEBUG: data shape:", data.observation.shape)
        total_loss, metrics = gradient_update_fn(model, data, loss_fn, optimizer, key)
        print("DEBUG: loss and metric:", total_loss, metrics)
        return total_loss, metrics

    def sgd_step(
        model: nnx.Module,
        optimizer: nnx.Optimizer,
        key: PRNGKey,
        data: types.Transition,
        # normalizer_params: running_statistics.RunningStatisticsState,
    ):

        # this step is to shuffle the data and cut them into several minibatches
        # The reason for this is the parallelized environment generate more data than a batch size, this
        # might reduced the overhead between the environment simulation and the training step.

        next_key, perm_key, loss_key = jax.random.split(key, 3)

        def convert_data(x: jnp.ndarray):
            x = jax.random.permutation(perm_key, x)
            # x = jnp.reshape(x, (config.num_minibatches, -1) + x.shape[1:])
            return x
        shuffled_data = jax.tree_util.tree_map(convert_data, data)
        # maybe need to add a batch dim to the key
        total_loss, metrics = gradient_update_fn(model, data, loss_fn, optimizer, key)
        
        return total_loss, metrics, next_key

    def training_step(
        model: nnx.Module,
        optimizer: nnx.Optimizer,
        state: Union[envs.State, envs_v1.State],
        key: PRNGKey,
    ) -> Tuple[Metrics, Metrics]:
        key_sgd, key_generate_unroll, new_key = jax.random.split(key, 3)
        # generate new experience data
        policy = ppo_network.policy
        def f(carry, unused_t):
            current_state, current_key = carry
            current_key, next_key = jax.random.split(current_key)
            next_state, data = acting.generate_unroll(
                env,
                current_state,
                policy,
                current_key,
                config.unroll_length,
                extra_fields=("truncation",),
            )
            return (next_state, next_key), data

        (state, _), data = jax.lax.scan(
            f,
            (state, key_generate_unroll),
            (),
            length=config.batch_size * config.num_minibatches // config.num_envs,
        )
        print("DEBUG: data.observation.shape before:", data.observation.shape)
        # Have leading dimensions (batch_size * num_minibatches, unroll_length)
        data = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 1, 2), data)
        data = jax.tree_util.tree_map(
            lambda x: jnp.reshape(x, (-1,) + x.shape[2:]), data
        )
        print("DEBUG: data.observation.shape after:", data.observation.shape)
        assert data.discount.shape[1:] == (config.unroll_length,)

        # complete optimization steps on that experience

        # TODO: normalization later
        # Update normalization params and normalize observations.
        # normalizer_params = running_statistics.update(
        #     training_state.normalizer_params,
        #     data.observation,
        #     pmap_axis_name=_PMAP_AXIS_NAME,
        # )

        # TODO: could be implemented in scan
        total_loss, metrics = None, None
        for _ in range(config.num_updates_per_batch):
            total_loss, metrics, key_sgd = sgd_step(model, optimizer, key_sgd, data)

        return total_loss, metrics

    @nnx.jit
    def training_epoch(
        model: nnx.Module, optimizer: nnx.Optimizer, state: envs.State, key: PRNGKey
    ) -> Tuple[envs.State, Metrics]:
        loss_metrics = None
        for _ in range(max(config.num_resets_per_eval, 1)):
            total_loss, loss_metrics = training_step(model, optimizer, state, key)
        loss_metrics = jax.tree_util.tree_map(jnp.mean, loss_metrics)
        return state, loss_metrics

    # training_epoch = nnx.vmap(training_epoch, in_axes=(None, None, 0, 0))

    # Note that this is NOT a pure jittable method.
    def training_epoch_with_timing(
        model: nnx.Module, optimizer: nnx.Optimizer, env_state: envs.State, key: PRNGKey
    ) -> Tuple[envs.State, Metrics]:
        nonlocal training_walltime
        t = time.time()
        env_state, metrics = training_epoch(model, optimizer, env_state, key)
        # env_state, metrics = _strip_weak_type(result)

        metrics = jax.tree_util.tree_map(jnp.mean, metrics)
        jax.tree_util.tree_map(lambda x: x.block_until_ready(), metrics)

        epoch_training_time = time.time() - t
        training_walltime += epoch_training_time
        sps = (
            num_training_steps_per_epoch * env_step_per_training_step * max(config.num_resets_per_eval, 1)
        ) / epoch_training_time
        metrics = {
            "training/sps": sps,
            "training/walltime": training_walltime,
            **{f"training/{name}": value for name, value in metrics.items()},
        }
        print("METRICS:", metrics)
        return (
            env_state,
            metrics,
        )  # pytype: disable=bad-return-type  # py311-upgrade

    # if not config.eval_env:
    #     eval_env = environment
    # if config.randomization_fn is not None:
    #     v_randomization_fn = functools.partial(
    #         config.randomization_fn, rng=jax.random.split(eval_key, config.num_eval_envs)
    #     )
    # eval_env = wrap_for_training(
    #     eval_env,
    #     episode_length=config.episode_length,
    #     action_repeat=config.action_repeat,
    #     randomization_fn=v_randomization_fn,
    # )

    # evaluator = acting.Evaluator(
    #     eval_env,
    #     functools.partial(make_policy, deterministic=config.deterministic_eval),
    #     num_eval_envs=config.num_eval_envs,
    #     episode_length=config.episode_length,
    #     action_repeat=config.action_repeat,
    #     key=eval_key,
    # )

    # # Run initial eval
    # metrics = {}
    # if process_id == 0 and config.num_evals > 1:
    #     metrics = evaluator.run_evaluation(
    #         _unpmap((training_state.normalizer_params, training_state.params.policy)),
    #         training_metrics={},
    #     )
    #     logging.info(metrics)
    #     config.progress_fn(0, metrics)

    training_metrics = {}
    training_walltime = 0
    current_step = 0
    num_evals_after_init = int(1e5)
    for it in range(num_evals_after_init):
        logging.info("starting iteration %s %s", it, time.time() - xt)

        for _ in range(max(config.num_resets_per_eval, 1)):
            # optimization
            epoch_key, local_key = jax.random.split(local_key)
            env_state, training_metrics = training_epoch_with_timing(
                ppo_network, optimizer, env_state, epoch_key
            )
            current_step = it
            print("TO_PROGRESS_FUNC:", current_step, training_metrics)
            config.progress_fn(current_step, training_metrics)
            key_envs = jax.vmap(lambda x, s: jax.random.split(x[0], s), in_axes=(0, None))(key_envs, key_envs.shape[1])
            # TODO: move extra reset logic to the AutoResetWrapper.
            if config.num_resets_per_eval > 0:
                env_state = reset_fn(key_envs)
                env_state = jax.tree_util.tree_map(lambda x: jnp.squeeze(x, axis=0), env_state)
            
        # if process_id == 0:
        #     # Run evals.
        #     metrics = evaluator.run_evaluation(
        #         _unpmap((training_state.normalizer_params, training_state.params.policy)),
        #         training_metrics,
        #     )
        #     logging.info(metrics)
        #     config.progress_fn(current_step, metrics)
        #     params = _unpmap((training_state.normalizer_params, training_state.params.policy))
        #     _, policy_params_fn_key = jax.random.split(policy_params_fn_key)
        #     config.policy_params_fn(
        #         current_step=current_step,
        #         make_policy=make_policy,
        #         params=params,
        #         policy_params_fn_key=policy_params_fn_key,
        #     )

    total_steps = current_step
    assert total_steps >= config.num_timesteps

    return None
