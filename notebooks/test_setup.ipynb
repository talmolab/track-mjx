{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test setup\n",
    "\n",
    "This package requires a lot of annoying dependencies that all need to talk to each\n",
    "other.\n",
    "\n",
    "This notebook aims to facilitate with troubleshooting these by testing whether they are accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nvidia-smi` / GPU drivers\n",
    "\n",
    "This tests whether the `nvidia-smi` binary is accessible and returns the correct driver\n",
    "versions.\n",
    "\n",
    "This should be available if the NVIDIA GPU driver was intalled with CUDA support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 12 00:08:58 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:17:00.0 Off |                    0 |\n",
      "|  0%   39C    P8             33W /  300W |       4MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "proc = subprocess.run(\"nvidia-smi\")\n",
    "\n",
    "if proc.returncode:\n",
    "    print(\"Error in running nvidia-smi:\", proc.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EGL renderer\n",
    "\n",
    "The following checks whether we can import MuJoCo with the EGL backend set.\n",
    "\n",
    "This requires GPU acceleration and that the NVIDIA EGL drivers are accessible.\n",
    "\n",
    "On an NVIDIA-based container runtime, this is enabled by setting the\n",
    "`NVIDIA_DRIVER_CAPABILITIES` environment variable to `all` when running the container.\n",
    "\n",
    "If this is not possible, the alternative is to use the `osmesa` renderer which runs\n",
    "(much more slowly) on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that we can import MuJoCo with EGL backend...\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "\n",
    "try:\n",
    "  print(\"Checking that we can import MuJoCo with EGL backend...\")\n",
    "  import mujoco\n",
    "  mujoco.MjModel.from_xml_string(\"<mujoco/>\")\n",
    "  print(\"Success!\")\n",
    "except Exception as e:\n",
    "    print(\"Error in importing mujoco:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `jax`\n",
    "\n",
    "The `jax` and `jaxlib` libraries can be a bit annoying to interface with the GPU. This\n",
    "checks whether they can correctly get imported and place a tensor on the GPU.\n",
    "\n",
    "It also tries to set the XLA flags to ensure that it can do GEMM acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.4.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5271/4154785211.py:14: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
      "  print(\"XLA bridge backend:\", jax.lib.xla_bridge.get_backend().platform_version)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLA bridge backend: PJRT C API\n",
      "cuda 12030\n",
      "JAX devices: [CudaDevice(id=0)]\n",
      "Device for allocated tensor: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "xla_flags = os.environ.get(\"XLA_FLAGS\", \"\")\n",
    "xla_flags += \"--xla_gpu_triton_gemm_any=True \"\n",
    "xla_flags += \"--xla_gpu_enable_triton_softmax_fusion=true \"\n",
    "os.environ[\"XLA_FLAGS\"] = xla_flags\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "\n",
    "# Print XLA bridge backend info\n",
    "print(\"XLA bridge backend:\", jax.lib.xla_bridge.get_backend().platform_version)\n",
    "\n",
    "# Print available devices\n",
    "print(\"JAX devices:\", jax.devices())\n",
    "\n",
    "# Test a simple GPU operation\n",
    "x = jnp.ones((100, 100))\n",
    "print(\"Device for allocated tensor:\", x.addressable_data(0).device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track_mjx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
