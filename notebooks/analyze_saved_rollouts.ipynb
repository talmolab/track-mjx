{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering from the saved rollout h5 file\n",
    "\n",
    "This notebook will show you how to load the saved rollout (stored in `.h5` file format), generated in `rollout_from_checkpoint.ipynb`, and create a rendering video from that, with further visual analysis pipeline to visualize the agents, i.e. temporal dynamics of the intentions. \n",
    "\n",
    "## Note\n",
    "\n",
    "This notebook won't have any mujoco components to it, since we are doing analysis directly on the saved rollout file, this notebook is dedicated to perform analysis on the saved rollout file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "from track_mjx.environment.task.multi_clip_tracking import MultiClipTracking\n",
    "from track_mjx.environment.walker.rodent import Rodent\n",
    "\n",
    "import mujoco\n",
    "from pathlib import Path\n",
    "from dm_control import mjcf as mjcf_dm\n",
    "from dm_control.locomotion.walkers import rescale\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "import multiprocessing as mp\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering Related Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_backend_context(func):\n",
    "    \"\"\"\n",
    "    Decorator to switch to a headless backend during function execution.\n",
    "    \"\"\"\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        orig_backend = matplotlib.get_backend()\n",
    "        matplotlib.use(\"Agg\")  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "        # Code to execute BEFORE the original function\n",
    "        result = func(*args, **kwargs)\n",
    "        # Code to execute AFTER the original function\n",
    "        plt.close(\"all\")  # Figure auto-closing upon backend switching is deprecated.\n",
    "        matplotlib.use(orig_backend)\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def render_from_saved_rollout(\n",
    "    rollout: dict,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Render a rollout from saved qposes.\n",
    "\n",
    "    Args:\n",
    "        rollout (dict): A dictionary containing the qposes of the reference and rollout trajectories.\n",
    "\n",
    "    Returns:\n",
    "        list: list of frames of the rendering\n",
    "    \"\"\"\n",
    "    qposes_ref, qposes_rollout = rollout[\"qposes_ref\"], rollout[\"qposes_rollout\"]\n",
    "    # need to change to the new xml file\n",
    "    pair_render_xml_path = (\n",
    "        \"/root/vast/scott-yang/track-mjx/track_mjx/environment/walker/assets/rodent/rodent_ghostpair_scale080.xml\"\n",
    "    )\n",
    "    # TODO: Make this ghost rendering walker agonist\n",
    "    root = mjcf_dm.from_path(pair_render_xml_path)\n",
    "    rescale.rescale_subtree(\n",
    "        root,\n",
    "        0.9 / 0.8,\n",
    "        0.9 / 0.8,\n",
    "    )\n",
    "\n",
    "    mj_model = mjcf_dm.Physics.from_mjcf_model(root).model.ptr\n",
    "    mj_model.opt.solver = {\n",
    "        \"cg\": mujoco.mjtSolver.mjSOL_CG,\n",
    "        \"newton\": mujoco.mjtSolver.mjSOL_NEWTON,\n",
    "    }[\"cg\"]\n",
    "    mj_model.opt.iterations = 6\n",
    "    mj_model.opt.ls_iterations = 6\n",
    "    mj_data = mujoco.MjData(mj_model)\n",
    "\n",
    "    # save rendering and log to wandb\n",
    "    mujoco.mj_kinematics(mj_model, mj_data)\n",
    "    renderer = mujoco.Renderer(mj_model, height=480, width=640)\n",
    "    frames = []\n",
    "    print(\"MuJoCo Rendering...\")\n",
    "    for qpos1, qpos2 in tqdm(zip(qposes_rollout, qposes_ref), total=len(qposes_rollout)):\n",
    "        mj_data.qpos = np.append(qpos1, qpos2)\n",
    "        mujoco.mj_forward(mj_model, mj_data)\n",
    "        renderer.update_scene(\n",
    "            mj_data,\n",
    "            camera=\"close_profile\",\n",
    "        )\n",
    "        pixels = renderer.render()\n",
    "        frames.append(pixels)\n",
    "    return frames\n",
    "\n",
    "\n",
    "def plot_pca_intention(\n",
    "    idx,\n",
    "    episode_start,\n",
    "    pca_projections: np.ndarray,\n",
    "    clip_idx: int,\n",
    "    feature_name: str,\n",
    "    n_components: int = 4,\n",
    "    terminated=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    plot pca intention progression of the episode\n",
    "    Args:\n",
    "        idx: the current timestep\n",
    "        episode_start: the start timestep of the episode\n",
    "        pca_projections: the pca projection of the episode, shape (timestep, n_components)\n",
    "        clip_idx: the clip index\n",
    "        feature_name: the feature name\n",
    "        n_components: the number of pca components to plot\n",
    "        ylim: the y-axis limit\n",
    "        terminated: whether the episode is terminated\n",
    "\n",
    "    \"\"\"\n",
    "    max_y = np.max(list(pca_projections[:, :n_components]))\n",
    "    min_y = np.min(list(pca_projections[:, :n_components]))\n",
    "    y_lim = (min_y - 0.2, max_y + 0.2)\n",
    "    window_size = 530\n",
    "    idx_in_this_episode = idx - episode_start  # the current timestep in this episode\n",
    "    plt.figure(figsize=(9.6, 4.8))\n",
    "    for pc_ind in range(n_components):\n",
    "        # Plot the PCA projection of the episode\n",
    "        plt.plot(\n",
    "            pca_projections[episode_start:idx, pc_ind],\n",
    "            label=f\"PC {pc_ind} ({pca.explained_variance_ratio_[pc_ind]*100:.1f}%)\",\n",
    "        )\n",
    "        plt.scatter(idx - episode_start, pca_projections[idx - 1, pc_ind])\n",
    "    if terminated:\n",
    "        # Mark the episode termination\n",
    "        plt.axvline(x=idx - episode_start, color=\"r\", linestyle=\"-\")\n",
    "        plt.text(\n",
    "            idx - episode_start - 8,  # Adjust the x-offset as needed\n",
    "            sum(y_lim) / 2,  # Adjust the y-position as needed\n",
    "            \"Episode Terminated\",\n",
    "            color=\"r\",\n",
    "            rotation=90,\n",
    "        )  # Rotate the text vertically\n",
    "    if idx_in_this_episode <= window_size:\n",
    "        plt.xlim(0, window_size)\n",
    "    else:\n",
    "        plt.xlim(idx_in_this_episode - window_size, idx_in_this_episode)  # dynamically move xlim as time progress\n",
    "    plt.ylim(*y_lim)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.title(f\"PCA {feature_name} Progression for Clip {clip_idx}\")  # TODO make it configurable\n",
    "    # Get the current figure\n",
    "    fig = plt.gcf()\n",
    "    # Create a canvas for rendering\n",
    "    canvas = FigureCanvasAgg(fig)\n",
    "    # Render the canvas to a buffer\n",
    "    canvas.draw()\n",
    "    s, (width, height) = canvas.print_to_buffer()\n",
    "    # Convert the buffer to a PIL Image\n",
    "    image = Image.frombytes(\"RGBA\", (width, height), s)\n",
    "    rgb_array = np.array(image.convert(\"RGB\"))\n",
    "    return rgb_array\n",
    "\n",
    "\n",
    "def render_with_pca_progression(\n",
    "    rollout: dict, pca_projections: np.ndarray, n_components: int = 4, feature_name: str = \"ctrl\"\n",
    "):\n",
    "    \"\"\"\n",
    "    render with the rewards progression graph concat alongside with the rendering\n",
    "    \"\"\"\n",
    "    frames_mujoco = render_from_saved_rollout(rollout)[1:]\n",
    "    # skip the first frame, since we don't have intention for the first frame\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use(\"Agg\")  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "    clip_idx = int(rollout[\"info\"][0][\"clip_idx\"])\n",
    "    worker = functools.partial(\n",
    "        plot_pca_intention,\n",
    "        episode_start=0,\n",
    "        clip_idx=clip_idx,\n",
    "        pca_projections=pca_embedded,\n",
    "        n_components=n_components,\n",
    "        feature_name=feature_name,\n",
    "    )\n",
    "    print(\"Rendering with PCA progression...\")\n",
    "    # Use multiprocessing to parallelize the rendering of the reward graph\n",
    "    with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "        frames_pca = pool.map(worker, range(len(rollout[\"qposes_rollout\"])))\n",
    "    concat_frames = []\n",
    "    episode_start = 0\n",
    "    # implement reset logics of the reward graph too.\n",
    "    print(\"Concatenating frames...\")\n",
    "    for idx, frame in tqdm(enumerate(frames_mujoco)):\n",
    "        concat_frames.append(np.hstack([frame, frames_pca[idx]]))\n",
    "    reward_plot = plot_pca_intention(\n",
    "        len(frames_mujoco) - 1, episode_start, pca_projections, clip_idx, feature_name, n_components, terminated=True\n",
    "    )\n",
    "    plt.close(\"all\")  # Figure auto-closing upon backend switching is deprecated.\n",
    "    matplotlib.use(orig_backend)\n",
    "    for _ in range(50):\n",
    "        concat_frames.append(np.hstack([frames_mujoco[-1], reward_plot]))  # create stoppage when episode terminates\n",
    "    return concat_frames\n",
    "\n",
    "\n",
    "def display_video(frames, framerate=30):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        frames (array): (n_frames, height, width, 3)\n",
    "        framerate (int): the framerate of the video\n",
    "    \"\"\"\n",
    "    height, width, _ = frames[0].shape\n",
    "    dpi = 70\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use(\"Agg\")  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "    plt.close(\"all\")  # Figure auto-closing upon backend switching is deprecated.\n",
    "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "\n",
    "    def update(frame):\n",
    "        im.set_data(frame)\n",
    "        return [im]\n",
    "\n",
    "    interval = 1000 / framerate\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames, interval=interval, blit=True, repeat=False)\n",
    "    return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the rollout file from the `.h5` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "\n",
    "def load_from_h5py(file, group_path=\"/\") -> dict:\n",
    "    \"\"\"\n",
    "    Load a pytree structure from an HDF5 file.\n",
    "\n",
    "    Args:\n",
    "        file (h5py.File): An open HDF5 file object.\n",
    "        group_path (str): The HDF5 group path to read data from.\n",
    "\n",
    "    Returns:\n",
    "        The reconstructed data structure.\n",
    "    \"\"\"\n",
    "    group = file[group_path]\n",
    "    if isinstance(group, h5py.Dataset):\n",
    "        return group[()]  # Read dataset value\n",
    "    elif isinstance(group, h5py.Group):\n",
    "        if all(k.isdigit() for k in group.keys()):  # Likely a list\n",
    "            return [load_from_h5py(file, f\"{group_path}/{k}\") for k in sorted(group.keys(), key=int)]\n",
    "        else:  # Dictionary-like group\n",
    "            return {k: load_from_h5py(file, f\"{group_path}/{k}\") for k in group.keys()}\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported group type: {type(group)}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "with h5py.File(\"/root/vast/scott-yang/rodent_rollout_info/data/clip_1.h5\", \"r\") as h5file:\n",
    "    rollout = load_from_h5py(h5file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly get out the activations\n",
    "with h5py.File(\"/root/vast/scott-yang/rodent_rollout_info/data/clip_1.h5\", \"r\") as h5file:\n",
    "    activations = load_from_h5py(h5file, group_path=\"/activations\")\n",
    "    intentions = [a[\"intention\"] for a in activations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations[0][\"decoder\"][\"layer_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: PCA for all intentions across rollout\n",
    "\n",
    "The following cell traverse though the recorded rollout `.h5` file in the directly, and parse out the intentions of each episode. All of the intentions vectors are aggregated into a single matrix for PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_data(group_path, keys: List[str], clip_idx: int):\n",
    "    \"\"\"\n",
    "    Get the aggregate data from the hdf5 file\n",
    "    \"\"\"\n",
    "    with h5py.File(f\"/root/vast/scott-yang/rodent_rollout_info/data/clip_{clip_idx}.h5\", \"r\") as h5file:\n",
    "        data = load_from_h5py(h5file, group_path=group_path)\n",
    "        for key in keys:\n",
    "            if type(data) == list and type(data[0]) == dict:\n",
    "                data = [d[key] for d in data]\n",
    "            elif type(data) == dict:\n",
    "                data = data[key]\n",
    "            else:\n",
    "                raise ValueError(\"Data structure not supported\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiProcessing SpeedUp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this will take 0.63 * 850 = 535 seconds = 8.9 minutes to run, if I run it in a ordinary for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# if I wanna get the activations for the decoder layer 0 of clip 1\n",
    "get_aggregate_data(\"/activations\", [\"decoder\", \"layer_0\"], 1)\n",
    "\n",
    "# this will take 0.63 * 850 = 535 seconds = 8.9 minutes to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, if I multiprocessing the IO call, we only need 26 seconds to complete the call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "work = functools.partial(get_aggregate_data, \"/activations\", [\"decoder\", \"layer_2\"])\n",
    "\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    activations = list(tqdm(pool.imap(work, range(842)), total=842))\n",
    "\n",
    "activations = np.vstack(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "pca = pca.fit(activations)\n",
    "print(np.cumsum(pca.explained_variance_ratio_[:10]))\n",
    "pca_embedded = pca.transform(activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialized the PCA result to disk\n",
    "\n",
    "In this way, we can directly load the pca object to do the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize PCA components to a dictionary\n",
    "pca_data = {\n",
    "    \"components_\": pca.components_.tolist(),\n",
    "    \"explained_variance_\": pca.explained_variance_.tolist(),\n",
    "    \"explained_variance_ratio_\": pca.explained_variance_ratio_.tolist(),\n",
    "    \"mean_\": pca.mean_.tolist(),\n",
    "    \"n_components_\": pca.n_components_,\n",
    "}\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"pca_decoder_activations_layer_2_rodent.json\", \"w\") as f:\n",
    "    json.dump(pca_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the PCA object from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pca_intentions_rodent.json\", \"r\") as f:\n",
    "    loaded_pca_data = json.load(f)\n",
    "\n",
    "# Reconstruct the PCA object\n",
    "pca = PCA(n_components=loaded_pca_data[\"n_components_\"])\n",
    "pca.components_ = np.array(loaded_pca_data[\"components_\"])\n",
    "pca.explained_variance_ = np.array(loaded_pca_data[\"explained_variance_\"])\n",
    "pca.explained_variance_ratio_ = np.array(loaded_pca_data[\"explained_variance_ratio_\"])\n",
    "pca.mean_ = np.array(loaded_pca_data[\"mean_\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_embedded = pca.transform(intentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_embedded[:, 0], pca_embedded[:, 1], cmap=\"tab20\", alpha=0.3)\n",
    "plt.xlabel(f\"PCA 1 {pca.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "plt.ylabel(f\"PCA 2 {pca.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "plt.title(\"PCA of intentions across all episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_id = 39\n",
    "with h5py.File(f\"/root/vast/scott-yang/rodent_rollout_info/data/clip_{clip_id}.h5\", \"r\") as h5file:\n",
    "    rollout = load_from_h5py(h5file)\n",
    "    # directly get out the activations\n",
    "    act = get_aggregate_data(\"/activations\", [\"decoder\", \"layer_1\"], clip_id)\n",
    "pca_embedded = pca.transform(act)\n",
    "frames = render_with_pca_progression(rollout, pca_embedded, n_components=5, feature_name=\"decoder layer 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video(frames, framerate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for i in tqdm(range(21, 842)):\n",
    "    with h5py.File(f\"/root/vast/scott-yang/rodent_rollout_info/data/clip_{clip_id}.h5\", \"r\") as h5file:\n",
    "        rollout = load_from_h5py(h5file)\n",
    "        intentions = [a[\"intention\"] for a in rollout[\"activations\"]]\n",
    "    pca_embedded = pca.transform(intentions)\n",
    "    frames = render_with_pca_progression(rollout, pca_embedded, n_components=5, feature_name=\"intention\")\n",
    "    imageio.mimsave(\n",
    "        f\"/root/vast/scott-yang/rodent_rollout_info/videos_intentions_pca/rodent_clip_intention_{i}.mp4\", frames, fps=30\n",
    "    )\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video(frames, framerate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Labels of each clip id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"/root/vast/scott-yang/vnl_ray/clips/all_snippets.h5\", \"r\") as h5file:\n",
    "    group = h5file[\"clip_0/walkers/walker_0\"]\n",
    "    if isinstance(group, h5py.Dataset):\n",
    "        print(group[()])  # Read dataset value\n",
    "    else:\n",
    "        print(group.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"/root/vast/scott-yang/vnl_ray/clips/all_snips.p\", \"rb\") as file:\n",
    "    all_snips = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_snips[\"snips_order\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track-mjx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
