{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Environment\n",
    "Need to ensure that control steps in data is the same as control steps in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "main_path = Path().resolve().parent\n",
    "if str(main_path) not in sys.path:\n",
    "    sys.path.append(str(main_path))\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.95\"\n",
    "os.environ[\"MUJOCO_GL\"] = \"glfw\"\n",
    "os.environ[\"XLA_FLAGS\"] = (\n",
    "    \"--xla_gpu_enable_triton_softmax_fusion=true --xla_gpu_triton_gemm_any=True \"\n",
    ")\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"glfw\"\n",
    "\n",
    "from absl import flags\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import uuid\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import functools\n",
    "import jax\n",
    "from typing import Dict\n",
    "import wandb\n",
    "import imageio\n",
    "from brax import envs\n",
    "from dm_control import mjcf as mjcf_dm\n",
    "from dm_control.locomotion.walkers import rescale\n",
    "\n",
    "import track_mjx.agent.custom_ppo as ppo\n",
    "from track_mjx.agent import custom_ppo\n",
    "from brax.io import model\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "from jax import numpy as jp\n",
    "\n",
    "from track_mjx.environment.task.multi_clip_tracking import MultiClipTracking\n",
    "from track_mjx.environment.task.single_clip_tracking import SingleClipTracking\n",
    "# from track_mjx.io.preprocess.mjx_preprocess import process_clip_to_train\n",
    "# from track_mjx.io import preprocess as preprocessing  # the pickle file needs it\n",
    "from track_mjx.environment import custom_wrappers\n",
    "from track_mjx.agent import custom_ppo_networks\n",
    "from track_mjx.agent.logging import setup_training_logging\n",
    "from track_mjx.io import load\n",
    "from track_mjx.environment.walker.rodent import Rodent\n",
    "from track_mjx.environment.walker.fly import Fly\n",
    "\n",
    "from track_mjx.environment.task.reward import RewardConfig\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "main_dir = Path().resolve().parent\n",
    "print(main_dir)\n",
    "if str(main_dir) not in sys.path:\n",
    "    sys.path.append(str(main_dir))\n",
    "    \n",
    "config_path = \"track_mjx/config\"\n",
    "data_path = main_dir / \"../stac-mjx/transform_snips.h5\"\n",
    "cfg = load.load_configs(main_dir / config_path, \"rodent-full-clips.yaml\")\n",
    "\n",
    "env_args = cfg[\"env_config\"][\"env_args\"]\n",
    "env_rewards = cfg[\"env_config\"][\"reward_weights\"]\n",
    "# train_cfg = cfg[\"train_setup\"][\"train_config\"]\n",
    "walker_cfg = cfg[\"walker_config\"]\n",
    "# traj_cfg = cfg[\"reference_config\"]\n",
    "walker_type = cfg[\"walker_type\"]\n",
    "traj_config = cfg[\"reference_config\"]\n",
    "try:\n",
    "    n_devices = jax.device_count(backend=\"gpu\")\n",
    "    print(f\"Using {n_devices} GPUs\")\n",
    "except:\n",
    "    n_devices = 1\n",
    "    print(\"Not using GPUs\")\n",
    "\n",
    "envs.register_environment(\"rodent_single_clip\", SingleClipTracking)\n",
    "envs.register_environment(\"rodent_multi_clip\", MultiClipTracking)\n",
    "envs.register_environment(\"fly_multi_clip\", MultiClipTracking)\n",
    "\n",
    "reference_clip = load.make_multiclip_data(data_path)\n",
    "\n",
    "walker_map = {\n",
    "    \"rodent\": Rodent,\n",
    "    \"fly\": Fly,\n",
    "}\n",
    "walker_class = walker_map[walker_type]\n",
    "walker = walker_class(**walker_cfg)\n",
    "\n",
    "reward_config = RewardConfig(\n",
    "    too_far_dist = env_rewards[\"too_far_dist\"],\n",
    "    bad_pose_dist = env_rewards[\"bad_pose_dist\"],\n",
    "    bad_quat_dist = env_rewards[\"bad_quat_dist\"],\n",
    "    ctrl_cost_weight = env_rewards[\"ctrl_cost_weight\"],\n",
    "    ctrl_diff_cost_weight = env_rewards[\"ctrl_diff_cost_weight\"],\n",
    "    pos_reward_weight = env_rewards[\"pos_reward_weight\"],\n",
    "    quat_reward_weight = env_rewards[\"quat_reward_weight\"],\n",
    "    joint_reward_weight = env_rewards[\"joint_reward_weight\"],\n",
    "    angvel_reward_weight = env_rewards[\"angvel_reward_weight\"],\n",
    "    bodypos_reward_weight = env_rewards[\"bodypos_reward_weight\"],\n",
    "    endeff_reward_weight = env_rewards[\"endeff_reward_weight\"],\n",
    "    healthy_z_range = env_rewards[\"healthy_z_range\"],\n",
    "    pos_reward_exp_scale = env_rewards[\"pos_reward_exp_scale\"],\n",
    "    quat_reward_exp_scale = env_rewards[\"quat_reward_exp_scale\"],\n",
    "    joint_reward_exp_scale = env_rewards[\"joint_reward_exp_scale\"],\n",
    "    angvel_reward_exp_scale = env_rewards[\"angvel_reward_exp_scale\"],\n",
    "    bodypos_reward_exp_scale = env_rewards[\"bodypos_reward_exp_scale\"],\n",
    "    endeff_reward_exp_scale = env_rewards[\"endeff_reward_exp_scale\"],\n",
    "    penalty_pos_distance_scale = jp.array(env_rewards[\"penalty_pos_distance_scale\"]),\n",
    ")\n",
    "\n",
    "env = envs.get_environment(\n",
    "    env_name=cfg[\"env_config\"][\"env_name\"],\n",
    "    reference_clip=reference_clip,\n",
    "    walker=walker,\n",
    "    reward_config=reward_config,\n",
    "    **env_args,\n",
    "    **traj_config,\n",
    ")\n",
    "\n",
    "print(\"Environment created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_env = custom_wrappers.AutoResetWrapperTracking(\n",
    "    custom_wrappers.RenderRolloutWrapperTracking(env)\n",
    "    )\n",
    "jit_reset = jax.jit(rollout_env.reset)\n",
    "jit_step = jax.jit(rollout_env.step)\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, key = jax.random.split(key)\n",
    "state = jit_reset(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = [state]\n",
    "frames = [env._get_cur_frame(state.info, state.pipeline_state)]\n",
    "for i in range(1000):\n",
    "    _, key = jax.random.split(key)\n",
    "    action = jax.random.normal(key, shape=(env.action_size,))\n",
    "    state = jit_step(state, action)\n",
    "    rollout.append(state)\n",
    "    frames.append(env._get_cur_frame(state.info, state.pipeline_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_metrics = np.array([s.metrics[\"fall\"] for s in rollout])\n",
    "too_far_metrics = np.array([s.metrics[\"too_far\"] for s in rollout])\n",
    "bad_pose_metrics = np.array([s.metrics[\"bad_pose\"] for s in rollout])\n",
    "bad_quat_metrics = np.array([s.metrics[\"bad_quat\"] for s in rollout])\n",
    "nan_metrics = np.array([s.metrics[\"nan\"] for s in rollout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the metrics as scatter plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "timesteps = range(len(rollout))\n",
    "\n",
    "plt.scatter(timesteps, fall_metrics, label=\"Fall Metrics\", alpha=0.6)\n",
    "plt.scatter(timesteps, too_far_metrics, label=\"Too Far Metrics\", alpha=0.6)\n",
    "plt.scatter(timesteps, bad_pose_metrics, label=\"Bad Pose Metrics\", alpha=0.6)\n",
    "plt.scatter(timesteps, bad_quat_metrics, label=\"Bad Quat Metrics\", alpha=0.6)\n",
    "plt.scatter(timesteps, nan_metrics, label=\"Nan Metrics\", alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.title(\"Metrics Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(frames)), frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = (fall_metrics + \n",
    "           too_far_metrics + \n",
    "           bad_pose_metrics + \n",
    "           bad_quat_metrics + \n",
    "           nan_metrics)\n",
    "print(overlap.shape)\n",
    "np.count_nonzero(np.array(overlap)>=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(np.array(frames)==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset(rng_key)\n",
    "state.pipeline_state.qpos[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_clip = jax.tree_map(\n",
    "            lambda x: x[state.info[\"cur_frame\"]], env._get_reference_clip(state.info)\n",
    "        )\n",
    "reference_clip.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
