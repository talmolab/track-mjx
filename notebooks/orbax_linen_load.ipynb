{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbax.checkpoint as ocp\n",
    "from track_mjx.agent.custom_ppo import TrainingState\n",
    "from track_mjx.agent import custom_losses as ppo_losses\n",
    "from track_mjx.agent import custom_ppo, custom_ppo_networks\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "from brax import envs\n",
    "from brax.training.acme import running_statistics, specs\n",
    "from track_mjx.environment import custom_wrappers\n",
    "\n",
    "from track_mjx.environment.task.multi_clip_tracking import MultiClipTracking\n",
    "from track_mjx.environment.task.single_clip_tracking import SingleClipTracking\n",
    "from track_mjx.environment.task.reward import RewardConfig\n",
    "\n",
    "from track_mjx.environment.walker.rodent import Rodent\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import OmegaConf\n",
    "import pickle\n",
    "import functools\n",
    "\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"./../track_mjx/config\")\n",
    "\n",
    "# Load the config file\n",
    "cfg = compose(config_name=\"rodent-full-clips\")\n",
    "\n",
    "# register the environment\n",
    "envs.register_environment(\"rodent_single_clip\", SingleClipTracking)\n",
    "envs.register_environment(\"rodent_multi_clip\", MultiClipTracking)\n",
    "envs.register_environment(\"fly_multi_clip\", MultiClipTracking)\n",
    "\n",
    "env_cfg = OmegaConf.to_container(cfg, resolve=True)\n",
    "env_args = cfg.env_config[\"env_args\"]\n",
    "env_rewards = cfg.env_config[\"reward_weights\"]\n",
    "train_config = cfg.train_setup[\"train_config\"]\n",
    "walker_config = cfg[\"walker_config\"]\n",
    "traj_config = cfg[\"reference_config\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `TrainingState`\n",
    "\n",
    "Currently I just directly copy and paste the scripts from the training code. In the future for the sake of runtime and cleaner code, we can have a helper function that construct the abstracted `TrainingState` directly without creating the actual environment. This is on Scott's TODOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(Scott): move this to track_mjx.io module\n",
    "input_data_path = \"/root/vast/scott-yang/track-mjx/data/ReferenceClip.p\"\n",
    "with open(input_data_path, \"rb\") as file:\n",
    "    reference_clip = pickle.load(file)\n",
    "\n",
    "walker_map = {\n",
    "    \"rodent\": Rodent,\n",
    "}\n",
    "\n",
    "walker_class = walker_map[env_cfg[\"walker_type\"]]\n",
    "walker = walker_class(**walker_config)\n",
    "\n",
    "# didn't use args** since penalty_pos_distance_scale need conversion\n",
    "reward_config = RewardConfig(\n",
    "    too_far_dist=env_rewards.too_far_dist,\n",
    "    bad_pose_dist=env_rewards.bad_pose_dist,\n",
    "    bad_quat_dist=env_rewards.bad_quat_dist,\n",
    "    ctrl_cost_weight=env_rewards.ctrl_cost_weight,\n",
    "    ctrl_diff_cost_weight=env_rewards.ctrl_diff_cost_weight,\n",
    "    pos_reward_weight=env_rewards.pos_reward_weight,\n",
    "    quat_reward_weight=env_rewards.quat_reward_weight,\n",
    "    joint_reward_weight=env_rewards.joint_reward_weight,\n",
    "    angvel_reward_weight=env_rewards.angvel_reward_weight,\n",
    "    bodypos_reward_weight=env_rewards.bodypos_reward_weight,\n",
    "    endeff_reward_weight=env_rewards.endeff_reward_weight,\n",
    "    healthy_z_range=env_rewards.healthy_z_range,\n",
    "    pos_reward_exp_scale=env_rewards.pos_reward_exp_scale,\n",
    "    quat_reward_exp_scale=env_rewards.quat_reward_exp_scale,\n",
    "    joint_reward_exp_scale=env_rewards.joint_reward_exp_scale,\n",
    "    angvel_reward_exp_scale=env_rewards.angvel_reward_exp_scale,\n",
    "    bodypos_reward_exp_scale=env_rewards.bodypos_reward_exp_scale,\n",
    "    endeff_reward_exp_scale=env_rewards.endeff_reward_exp_scale,\n",
    "    penalty_pos_distance_scale=jnp.array(env_rewards.penalty_pos_distance_scale),\n",
    ")\n",
    "\n",
    "# Automatically match dict keys and func needs\n",
    "environment = envs.get_environment(\n",
    "    env_name=cfg.env_config.env_name,\n",
    "    reference_clip=reference_clip,\n",
    "    walker=walker,\n",
    "    reward_config=reward_config,\n",
    "    **env_args,\n",
    "    **traj_config,\n",
    ")\n",
    "# Episode length is equal to (clip length - random init range - traj length) * steps per cur frame.\n",
    "# Will work on not hardcoding these values later\n",
    "episode_length = (\n",
    "    traj_config.clip_length - traj_config.random_init_range - traj_config.traj_length\n",
    ") * environment._steps_for_cur_frame\n",
    "print(f\"episode_length {episode_length}\")\n",
    "\n",
    "network_factory = functools.partial(\n",
    "    custom_ppo_networks.make_intention_ppo_networks,\n",
    "    encoder_hidden_layer_sizes=tuple(cfg.network_config.encoder_layer_sizes),\n",
    "    decoder_hidden_layer_sizes=tuple(cfg.network_config.decoder_layer_sizes),\n",
    "    value_hidden_layer_sizes=tuple(cfg.network_config.critic_layer_sizes),\n",
    ")\n",
    "\n",
    "seed = 42\n",
    "\n",
    "key = jax.random.PRNGKey(seed)\n",
    "global_key, local_key = jax.random.split(key)\n",
    "local_key, key_env, eval_key = jax.random.split(local_key, 3)\n",
    "# key_networks should be global, so that networks are initialized the same\n",
    "# way for different processes.\n",
    "key_policy, key_value, policy_params_fn_key = jax.random.split(global_key, 3)\n",
    "\n",
    "v_randomization_fn = None\n",
    "\n",
    "if isinstance(environment, envs.Env):\n",
    "    wrap_for_training = custom_wrappers.wrap\n",
    "else:\n",
    "    wrap_for_training = custom_wrappers.wrap\n",
    "\n",
    "train_env = wrap_for_training(\n",
    "    environment,\n",
    "    episode_length=episode_length,\n",
    "    action_repeat=1,\n",
    "    randomization_fn=v_randomization_fn,\n",
    ")\n",
    "\n",
    "reset_fn = train_env.reset\n",
    "key_envs = jax.random.split(key_env, 1)\n",
    "env_state = reset_fn(key_envs)\n",
    "\n",
    "normalize = lambda x, y: x\n",
    "if True:\n",
    "    normalize = running_statistics.normalize\n",
    "ppo_network = network_factory(\n",
    "    env_state.obs.shape[-1],\n",
    "    int(env_state.info[\"reference_obs_size\"][0]),\n",
    "    train_env.action_size,\n",
    "    preprocess_observations_fn=normalize,\n",
    ")\n",
    "make_policy = custom_ppo_networks.make_inference_fn(ppo_network)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=1e-4)\n",
    "\n",
    "init_params = ppo_losses.PPONetworkParams(\n",
    "    policy=ppo_network.policy_network.init(key_policy),\n",
    "    value=ppo_network.value_network.init(key_value),\n",
    ")\n",
    "\n",
    "training_state = TrainingState(  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
    "    optimizer_state=optimizer.init(init_params),  # pytype: disable=wrong-arg-types  # numpy-scalars\n",
    "    params=init_params,\n",
    "    normalizer_params=running_statistics.init_state(specs.Array(env_state.obs.shape[-1:], jnp.dtype(\"float32\"))),\n",
    "    env_steps=0,\n",
    ")\n",
    "\n",
    "abstract_policy = (training_state.normalizer_params, training_state.params.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orbax Checkpoint Related\n",
    "\n",
    "The following code will create the checkpoint manager and restore the checkpoint for both the policy and train state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore the whole model (both policy module, and training state)\n",
    "options = ocp.CheckpointManagerOptions(step_prefix=\"PPONetwork\")\n",
    "with ocp.CheckpointManager(\n",
    "    \"/root/vast/scott-yang/track-mjx/model_checkpoints/rodent_data/ReferenceClip.p_c52ed4\",\n",
    "    options=options,\n",
    ") as mngr:\n",
    "    print(f\"latest checkpoint step: {mngr.latest_step()}\")\n",
    "    policy = mngr.restore(\n",
    "        mngr.latest_step(), args=ocp.args.Composite(policy=ocp.args.StandardRestore(abstract_policy))\n",
    "    )[\"policy\"]\n",
    "    train_state = mngr.restore(\n",
    "        mngr.latest_step(), args=ocp.args.Composite(train_state=ocp.args.StandardRestore(training_state))\n",
    "    )[\"train_state\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering from the loaded checkpoint\n",
    "\n",
    "For this, I created a simplified version of the `policy_param_fn`, which just render the videos and save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "def display_video(frames, framerate=30):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        frames (array): (n_frames, height, width, 3)\n",
    "        framerate (int)\n",
    "    \"\"\"\n",
    "    height, width, _ = frames[0].shape\n",
    "    dpi = 70\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use(\"Agg\")  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "    plt.close(\"all\")  # Figure auto-closing upon backend switching is deprecated.\n",
    "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "\n",
    "    def update(frame):\n",
    "        im.set_data(frame)\n",
    "        return [im]\n",
    "\n",
    "    interval = 1000 / framerate\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames, interval=interval, blit=True, repeat=False)\n",
    "    return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from track_mjx.agent.logging import render_rollout\n",
    "from IPython.display import HTML\n",
    "\n",
    "frames = render_rollout(mngr.latest_step(), make_policy, policy, key_policy, cfg, environment, \".\")\n",
    "\n",
    "display_video(frames, framerate=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track_mjx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
