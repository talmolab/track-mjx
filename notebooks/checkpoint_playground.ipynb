{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `track-mjx` - Checkpoint Playground\n",
    "\n",
    "This notebook is a playground for the saved checkpoints of `track-mjx`'s kinematic replay tasks. It allows you to load the intention network and perform different operations on it, such as inspecting the decoder with random intentions, generating a trajectory from a given intention, evaluating model performance with various metrics, visualizing the trajectories and intention distributions, and implementing additional analysis tools to enhance the exploration of model behavior and capabilities.\n",
    "\n",
    "## !! JAX's Sharp Tips !!\n",
    "\n",
    "If you want to modify the input of a `jax.jit` compiled function, please do make sure that the input arguments have consistent types across the notebook. For consistency, this notebook uses `jnp.array` as the default type for all inputs. If you want to use `np.array`, please make sure that the input arguments are all `jnp.array`, or consistent with previous compilation time.\n",
    "\n",
    "If you trigger excessive re-compilation, please review the input types and ensure they are properly aligned with previous compiled functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Send logging outputs to stdout (comment this out if preferred)\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.INFO)\n",
    "\n",
    "# Change this to egl or glfw if available\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "\n",
    "# This is important when you have multiple notebook that is using JAX on the same machine\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = os.environ.get(\n",
    "    \"XLA_PYTHON_CLIENT_MEM_FRACTION\", \"0.4\"\n",
    ")\n",
    "\n",
    "\n",
    "from track_mjx.agent import checkpointing\n",
    "from track_mjx.agent.mlp_ppo.intention_network import Decoder\n",
    "from track_mjx.analysis.rollout import create_environment\n",
    "from track_mjx.environment.wrappers import RenderRolloutWrapperMulticlipTracking\n",
    "from track_mjx.analysis.render import render_rollout, display_video\n",
    "from track_mjx.analysis.utils import save_to_h5py, load_from_h5py\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "# enable JAX persistent compilation cache\n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n",
    "jax.config.update(\"jax_persistent_cache_min_entry_size_bytes\", -1)\n",
    "jax.config.update(\"jax_persistent_cache_min_compile_time_secs\", 0)\n",
    "jax.config.update(\"jax_persistent_cache_enable_xla_caches\", \"xla_gpu_per_fusion_autotune_cache_dir\")\n",
    "\n",
    "from brax.training import distribution\n",
    "from brax.training.acme import running_statistics\n",
    "from pathlib import Path\n",
    "\n",
    "# rendering related\n",
    "import mujoco\n",
    "from dm_control import mjcf as mjcf_dm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from dm_control.locomotion.walkers import rescale\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from PIL import Image\n",
    "import multiprocessing as mp\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Network Initialization and Checkpoint Loading\n",
    "\n",
    "First, we need to load the checkpoint. If you want to load your own custom checkpoint, you can replace the `ckpt_path` variable to your checkpoint path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with your checkpoint path\n",
    "ckpt_path = \"/root/vast/scott-yang/track-mjx/model_checkpoints/250701_232752_537208\"\n",
    "# Load config from checkpoint\n",
    "ckpt = checkpointing.load_checkpoint_for_eval(ckpt_path)\n",
    "\n",
    "cfg = ckpt[\"cfg\"]\n",
    "\n",
    "# make some changes to the config\n",
    "# replace with absolute path to your data\n",
    "# -- your notebook may not have access to the same relative path\n",
    "cfg.data_path = \"/root/vast/scott-yang/track-mjx/data/transform_snips.h5\"\n",
    "cfg.train_setup.checkpoint_to_restore = ckpt_path\n",
    "\n",
    "# NOTE: To use accelerated JAX.JIT, only run the following code only once.\n",
    "# If you re-run, you will triggers recompilations\n",
    "\n",
    "env = create_environment(cfg)\n",
    "env = RenderRolloutWrapperMulticlipTracking(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code initializes the network and loads the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the abstract decoder\n",
    "network_config = cfg[\"network_config\"]\n",
    "\n",
    "# initialize the decoder with last layer represent the mean and variance of the action distribution\n",
    "decoder = Decoder(\n",
    "    network_config[\"decoder_layer_sizes\"] + [network_config[\"action_size\"] * 2]\n",
    ")\n",
    "\n",
    "normalizer = running_statistics.normalize\n",
    "# load the normalizer parameters\n",
    "normalizer_param = ckpt[\"policy\"][0]\n",
    "\n",
    "# load the decoder parameters\n",
    "decoder_raw = ckpt[\"policy\"][1][\"params\"][\"decoder\"]\n",
    "decoder_param = {\"params\": decoder_raw}\n",
    "# initialize the action distribution\n",
    "action_distribution = distribution.NormalTanhDistribution(\n",
    "    event_size=network_config[\"action_size\"]\n",
    ")\n",
    "\n",
    "# prevent recompilation\n",
    "jit_env_reset = jax.jit(env.reset, static_argnames=(\"clip_idx\",))\n",
    "jit_env_step = jax.jit(env.step)\n",
    "jit_apply = jax.jit(decoder.apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test run: feed decoder with zero intentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize state and random key.\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, env_rng = jax.random.split(rng)\n",
    "rng, init_key = jax.random.split(rng)\n",
    "\n",
    "state = jit_env_reset(rng=env_rng, clip_idx=41)\n",
    "proprioceptive_obs_size, reference_obs_size, intention_size = (\n",
    "    state.info[\"proprioceptive_obs_size\"],\n",
    "    state.info[\"reference_obs_size\"],\n",
    "    cfg[\"network_config\"][\"intention_size\"],\n",
    ")\n",
    "\n",
    "actions = action_distribution.mode(\n",
    "    jit_apply(decoder_param, jnp.zeros(proprioceptive_obs_size + intention_size))[0]\n",
    ")\n",
    "print(\"Number of actions:\", len(actions))\n",
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX Tips:\n",
    "\n",
    "When performing training, it’s recommended to fully leverage JAX’s functionality, such as using `jax.lax.scan` to replace Python-native for loops. This brings the entire loop into the computation graph, allowing JAX to optimize it more effectively.\n",
    "\n",
    "However, during inference and analysis, it’s often better to avoid compiling the entire function — including the `for` loop — all at once. Instead, you can compile individual sub-functions and call those smaller, compiled functions within a regular Python for loop. This approach is preferable because inference and analysis workflows often involve frequent modifications to the rollout functions. By keeping the loop outside the compiled graph, you can avoid unnecessary recompilation of the entire loop when only a small part of the algorithm changes.\n",
    "\n",
    "!! To avoid costly recompilation, make sure your input parameter to the jitted function is the same type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Decoder with Random Intentions\n",
    "\n",
    "In this section, we will explore how the decoder behaves with random intentions, compared with the randomly generated actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize state and random key.\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, env_rng = jax.random.split(rng)\n",
    "rng, init_key = jax.random.split(rng)\n",
    "\n",
    "state = jit_env_reset(rng=env_rng, clip_idx=41)\n",
    "\n",
    "qposes_rollout = []\n",
    "\n",
    "for _ in range(500):\n",
    "    # Generate intentions using JAX’s random for consistency\n",
    "    # normalize the observation\n",
    "    obs = normalizer(state.obs, normalizer_param)\n",
    "    rng, subkey = jax.random.split(rng)\n",
    "    intentions = jax.random.normal(subkey, shape=(intention_size,))\n",
    "    # Concatenate intentions with a slice of the observation.\n",
    "    concatenated = jnp.concatenate(\n",
    "        [intentions, obs[..., cfg[\"network_config\"][\"reference_obs_size\"] :]],\n",
    "        axis=-1,\n",
    "    )\n",
    "    # Get the decoder output and sample an action.\n",
    "    logits = jit_apply(decoder_param, concatenated)[0]\n",
    "    rng, subkey = jax.random.split(rng)\n",
    "    # NOTE: This action returns as np array, from brax implementations\n",
    "    # and this triggers the recompilation of jax.jit\n",
    "    action = jnp.array(action_distribution.mode(logits))\n",
    "    qposes_rollout.append(state.pipeline_state.qpos)\n",
    "    # Step the environment.\n",
    "    state = jit_env_step(state, action)\n",
    "\n",
    "rollout = {\"qposes_rollout\": jnp.array(qposes_rollout)}\n",
    "\n",
    "# render the first 200 steps\n",
    "frames, render_fps = render_rollout(cfg, rollout, render_ghost=False)\n",
    "print(\"Rollout videos with random intentions\")\n",
    "display_video(frames, framerate=render_fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first run will take ~7 minutes, second run takes about 20s seconds on an A40 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize state and random key.\n",
    "rng, env_rng = jax.random.split(rng)\n",
    "rng, init_key = jax.random.split(rng)\n",
    "\n",
    "state = jit_env_reset(rng=env_rng, clip_idx=41)\n",
    "\n",
    "qposes_rollout = []\n",
    "\n",
    "for _ in range(500):\n",
    "    # Generate intentions using JAX’s random for consistency\n",
    "    rng, subkey = jax.random.split(rng)\n",
    "    action = jax.random.normal(subkey, shape=(38,))\n",
    "    qposes_rollout.append(state.pipeline_state.qpos)\n",
    "    # Step the environment.\n",
    "    state = jit_env_step(state, action)\n",
    "\n",
    "rollout = {\"qposes_rollout\": jnp.array(qposes_rollout)}\n",
    "\n",
    "# render the first 200 steps\n",
    "frames, render_fps = render_rollout(cfg, rollout, render_ghost=False)\n",
    "# take a look at the video\n",
    "print(\"Rollout with random actions\")\n",
    "display_video(frames, framerate=render_fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the rendering of the decoder actions with random intentions is not very good, but still better than pure random actions that it remains upright and did not fall over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Recreate the Recorded Rollout\n",
    "\n",
    "The following cell recreates the recorded rollout based on the recorded statistics taken during the saved rollout. We will explore different ways of recreating the recorded rollout, including:\n",
    "\n",
    "**Note:** Please get the saved rollout file from the `rollout_from_checkpoint.ipynb` file, with the `clip_idx` set to `41`\n",
    "\n",
    "- 2.0: Load the saved rollout\n",
    "- 2.1: Step with recorded **actions**\n",
    "- 2.2: Step with recorded **decoder inputs**\n",
    "- 2.3: Step with recorded **intentions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some rendering related code\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def visualize_ego_observation(\n",
    "    ego_obs, window_size, title, frame, feature_mask: List[int] | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Render one frame of the sliding window plot.\n",
    "\n",
    "    For frame indices less than window_size, it displays the data from index 0 to current index,\n",
    "    but fixes the x-axis from 0 to window_size. For later frames, the most recent window_size time\n",
    "    steps are displayed. A red vertical line indicates the current time index.\n",
    "\n",
    "    Args:\n",
    "        ego_obs: A 2D numpy array of shape (147, N) representing the ego observation.\n",
    "        window_size: The size of the sliding window.\n",
    "        frame: The current frame index.\n",
    "\n",
    "    Returns:\n",
    "        rgb_array: A numpy array of shape (480, 640, 3) representing the rendered image.\n",
    "    \"\"\"\n",
    "    if feature_mask is not None:\n",
    "        ego_obs = ego_obs[feature_mask[0] : feature_mask[1], :]\n",
    "    # Compute global min and max to keep colormap consistent\n",
    "    global_vmin = float(ego_obs.min())\n",
    "    global_vmax = float(ego_obs.max())\n",
    "\n",
    "    current_idx = frame + 1  # current time step (1-indexed)\n",
    "\n",
    "    # Create the figure with fixed dimensions: 6.4x4.8 inches at dpi=100 gives 640x480 pixels.\n",
    "    fig, ax = plt.subplots(figsize=(6.4, 4.8), dpi=100)\n",
    "    plt.tight_layout()\n",
    "    canvas = FigureCanvas(fig)\n",
    "\n",
    "    if current_idx < window_size:\n",
    "        # For early frames, plot data from 0 to current_idx.\n",
    "        window_data = ego_obs[:, :current_idx]  # shape: (147, current_idx)\n",
    "        # Set extent so the data covers [0, current_idx] in x,\n",
    "        # but later we force the x-axis to show 0 to window_size.\n",
    "        extent = [0, current_idx, 0, ego_obs.shape[0]]\n",
    "        ax.imshow(\n",
    "            window_data,\n",
    "            aspect=\"auto\",\n",
    "            interpolation=\"none\",\n",
    "            origin=\"lower\",\n",
    "            extent=extent,\n",
    "            vmin=global_vmin,\n",
    "            vmax=global_vmax,\n",
    "        )\n",
    "        # Fix x-axis to [0, window_size]\n",
    "        ax.set_xlim(0, window_size)\n",
    "        # Draw red vertical line at the current index\n",
    "        ax.axvline(x=current_idx, color=\"red\", linewidth=2)\n",
    "    else:\n",
    "        # For later frames, display the most recent window_size time steps.\n",
    "        window_data = ego_obs[\n",
    "            :, current_idx - window_size : current_idx\n",
    "        ]  # shape: (147, window_size)\n",
    "        extent = [current_idx - window_size, current_idx, 0, ego_obs.shape[0]]\n",
    "        ax.imshow(\n",
    "            window_data,\n",
    "            aspect=\"auto\",\n",
    "            interpolation=\"none\",\n",
    "            origin=\"lower\",\n",
    "            extent=extent,\n",
    "            vmin=global_vmin,\n",
    "            vmax=global_vmax,\n",
    "        )\n",
    "        # Even here, we fix the view from 0 to window_size (the window length).\n",
    "        ax.set_xlim(current_idx - window_size, current_idx)\n",
    "\n",
    "        ax.axvline(x=current_idx, color=\"red\", linewidth=2)\n",
    "    # ax.yaxis.set_visible(False)\n",
    "    # This hides the entire y-axis including ticks and labels\n",
    "    ax.set_title(title)\n",
    "    # Render the figure and convert to a NumPy RGB array using the proper canvas method.\n",
    "    canvas.draw()\n",
    "    s, (width, height) = canvas.print_to_buffer()\n",
    "    image = Image.frombytes(\"RGBA\", (width, height), s)\n",
    "    rgb_array = np.array(image.convert(\"RGB\"))\n",
    "    plt.close(fig)\n",
    "    return rgb_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.0: Load the saved rollout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the rollout data, generated in rollout_from_checkpoint.ipynb\n",
    "example_rollout_path = (\n",
    "    \"/root/vast/scott-yang/track-mjx/model_checkpoints/250227_200156/rollout_41.h5\"\n",
    ")\n",
    "\n",
    "rollout_data = load_from_h5py(example_rollout_path)\n",
    "\n",
    "# render the saved rollout\n",
    "frames, realtime_framerate = render_rollout(cfg, rollout_data)\n",
    "display_video(frames, framerate=realtime_framerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1: Step with Recorded Actions\n",
    "\n",
    "The following cell step through the MuJoCo environment based on the recorded action taken during the saved rollout. In other words, the next for loop re-step through the physic simulator based on the recorded actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize state and random key.\n",
    "rng, env_rng = jax.random.split(rng)\n",
    "rng, init_key = jax.random.split(rng)\n",
    "\n",
    "state = jit_env_reset(rng=env_rng, clip_idx=41)\n",
    "\n",
    "qposes = []\n",
    "\n",
    "# make sure that the array is the same input type that gets feed into the jit_env_step\n",
    "ctrl = jnp.array(rollout_data[\"ctrl\"])\n",
    "\n",
    "for action in rollout_data[\"ctrl\"]:\n",
    "    state = jit_env_st4ep(state, action)\n",
    "    qposes.append(state.pipeline_state.qpos)\n",
    "\n",
    "frames = render_rollout(jnp.array(qposes))\n",
    "print(\"Rollout with the recorded actions\")\n",
    "display_video(frames, framerate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "ctrl = np.array(ctrl)\n",
    "window_size = 100\n",
    "total_time = ctrl.shape[1]  # should be 499\n",
    "title = \"Actions\"\n",
    "# Use multiprocessing to parallelize the rendering.\n",
    "f = functools.partial(visualize_ego_observation, ctrl, window_size, title)\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    frames_plt_1 = pool.map(f, list(range(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_video = np.concatenate((frames, frames_plt_1), axis=2)\n",
    "display_video(combined_video, framerate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the video we can see that it generates a somewhat similar trajectory as the recorded one, but the rodent eventually fall over. This is because we are not using the same random seed for the mujoco simulator, and the physics engine is not deterministic. In order to make it deterministic, we need to set the random seed for the mujoco simulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: Step with Recorded Decoder Inputs\n",
    "\n",
    "The following cell steps through the MuJoCo environment based on the recorded decoder inputs taken during the saved rollout. In other words, the next for loop re-steps through the physics simulator based on the recorded decoder inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = rollout_data[\"activations\"][\"decoder_inputs\"]\n",
    "inps = jnp.array(inps)\n",
    "\n",
    "\n",
    "state = jit_env_reset(rng=env_rng, clip_idx=41)\n",
    "qposes = []\n",
    "actual_ego_obs = []\n",
    "\n",
    "for inp in inps:\n",
    "    obs = normalizer(state.obs, normalizer_param)\n",
    "    actual_ego_obs.append(obs[..., cfg[\"network_config\"][\"reference_obs_size\"] :])\n",
    "    logits = jit_apply(decoder_param, inp)\n",
    "    action = jnp.array(action_distribution.mode(logits))\n",
    "    state = jit_env_step(state, action)\n",
    "    qposes.append(state.pipeline_state.qpos)\n",
    "\n",
    "actual_ego_obs = np.array(actual_ego_obs).T\n",
    "frames_mj = render_rollout(jnp.array(qposes))\n",
    "print(\"Rollout videos with decoder inputs\")\n",
    "# display_video(frames_mj, framerate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_obs = rollout_data[\"activations\"][\"decoder_inputs\"][..., 60:].T\n",
    "\n",
    "# Parameters\n",
    "window_size = 100\n",
    "total_time = ego_obs.shape[1]  # should be 499\n",
    "title = \"Recorded EgoObs (Offline EgoObs) (Feed into Decoder)\"\n",
    "# Use multiprocessing to parallelize the rendering.\n",
    "f = functools.partial(visualize_ego_observation, ego_obs, window_size, title)\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    frames_plt = pool.map(f, list(range(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "window_size = 100\n",
    "total_time = actual_ego_obs.shape[1]  # should be 499\n",
    "title = \"Actual EgoObs (Online EgoObs) (Discarded)\"\n",
    "# Use multiprocessing to parallelize the rendering.\n",
    "f = functools.partial(visualize_ego_observation, actual_ego_obs, window_size, title)\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    frames_plt_1 = pool.map(f, list(range(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_video = np.concatenate((frames_mj, frames_plt, frames_plt_1), axis=2)\n",
    "display_video(combined_video, framerate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "highlights the features that are related to the gait cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_obs = rollout_data[\"activations\"][\"decoder_inputs\"][..., 60:].T\n",
    "\n",
    "# Parameters\n",
    "window_size = 100\n",
    "total_time = ego_obs.shape[1]  # should be 499\n",
    "title = \"Recorded EgoObs (Offline EgoObs) (Feed into Decoder)\"\n",
    "feature_mask = [75, 100]\n",
    "# Use multiprocessing to parallelize the rendering.\n",
    "f = functools.partial(\n",
    "    visualize_ego_observation, ego_obs, window_size, title, feature_mask=feature_mask\n",
    ")\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    frames_plt = pool.map(f, list(range(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "window_size = 100\n",
    "total_time = actual_ego_obs.shape[1]  # should be 499\n",
    "title = \"Actual EgoObs (Online EgoObs) (Discarded)\"\n",
    "feature_mask = [75, 100]\n",
    "# Use multiprocessing to parallelize the rendering.\n",
    "f = functools.partial(\n",
    "    visualize_ego_observation,\n",
    "    actual_ego_obs,\n",
    "    window_size,\n",
    "    title,\n",
    "    feature_mask=feature_mask,\n",
    ")\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    frames_plt_1 = pool.map(f, list(range(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_video = np.concatenate((frames_mj, frames_plt, frames_plt_1), axis=2)\n",
    "display_video(combined_video, framerate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3: Step with Recorded Intentions\n",
    "\n",
    "The following cell step through the MuJoCo environment based on the recorded intentions taken during the saved rollout. In other words, the next for loop re-step through the physic simulator based on the recorded intentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qposes = []\n",
    "state = jit_env_reset(rng=env_rng, clip_idx=41)\n",
    "intentions = jnp.array(rollout_data[\"activations\"][\"intention\"])\n",
    "ego_obs = []\n",
    "actions = []\n",
    "\n",
    "for intention in intentions:\n",
    "    obs = normalizer(state.obs, normalizer_param)\n",
    "    ego_obs.append(obs[..., cfg[\"network_config\"][\"reference_obs_size\"] :])\n",
    "    concatenated = jnp.concatenate(\n",
    "        [intention, obs[..., cfg[\"network_config\"][\"reference_obs_size\"] :]],\n",
    "        axis=-1,\n",
    "    )\n",
    "    logits = jit_apply(decoder_param, concatenated)\n",
    "    action = jnp.array(action_distribution.mode(logits))\n",
    "    actions.append(action)\n",
    "    state = jit_env_step(state, action)\n",
    "    qposes.append(state.pipeline_state.qpos)\n",
    "\n",
    "actions = np.array(actions).T\n",
    "ego_obs = np.array(ego_obs).T\n",
    "frames_mj = render_rollout(jnp.array(qposes))\n",
    "print(\"Rollout videos with recorded intention\")\n",
    "# display_video(frames_mj, framerate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dummy data. Replace this with your actual ego_obs.\n",
    "# ego_obs should be a numpy array of shape (147, 499) (features x time).\n",
    "\n",
    "# Parameters\n",
    "window_size = 100\n",
    "total_time = ego_obs.shape[1]  # should be 499\n",
    "title = \"Online EgoObs (Feed into Decoder)\"\n",
    "# Use multiprocessing to parallelize the rendering.\n",
    "f = functools.partial(visualize_ego_observation, ego_obs, window_size, title)\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    frames_plt = pool.map(f, list(range(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "window_size = 100\n",
    "total_time = actions.shape[1]  # should be 499\n",
    "title = \"Actions\"\n",
    "# Use multiprocessing to parallelize the rendering.\n",
    "f = functools.partial(visualize_ego_observation, actions, window_size, title)\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    frames_plt_1 = pool.map(f, list(range(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_video = np.concatenate((frames_mj, frames_plt, frames_plt_1), axis=2)\n",
    "display_video(combined_video, framerate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "focus on the gait related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dummy data. Replace this with your actual ego_obs.\n",
    "# ego_obs should be a numpy array of shape (147, 499) (features x time).\n",
    "\n",
    "# Parameters\n",
    "window_size = 100\n",
    "total_time = ego_obs.shape[1]  # should be 499\n",
    "title = \"Online EgoObs (Feed into Decoder)\"\n",
    "feature_mask = [75, 100]\n",
    "# Use multiprocessing to parallelize the rendering.\n",
    "f = functools.partial(\n",
    "    visualize_ego_observation, ego_obs, window_size, title, feature_mask=feature_mask\n",
    ")\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    frames_plt = pool.map(f, list(range(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_video = np.concatenate((frames_mj, frames_plt), axis=2)\n",
    "display_video(combined_video, framerate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is remarkable! The rodent is able to perform the locomotion task more stably than the recorded actions and decoder inputs. This is because, with the sensory feedback loop provided by egocentric observations fed back into the decoder, the low-level controller can continuously adjust locomotion based on real-time sensory feedback. This highlights the importance of the sensorimotor loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Step with concatenated intentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = render_rollout(jnp.array(qposes)[130:230])\n",
    "print(\"Rollout videos with recorded intention from 130-230\")\n",
    "display_video(frames, framerate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qposes = []\n",
    "state = jit_env_reset(rng=env_rng, clip_idx=41)\n",
    "\n",
    "\n",
    "intentions = jnp.array(rollout_data[\"activations\"][\"intention\"])[130:230]\n",
    "ego_obs = []\n",
    "\n",
    "for _ in range(20):\n",
    "    for intention in intentions:\n",
    "        obs = normalizer(state.obs, normalizer_param)\n",
    "        ego_obs.append(obs[..., cfg[\"network_config\"][\"reference_obs_size\"] :])\n",
    "        concatenated = jnp.concatenate(\n",
    "            [intention, obs[..., cfg[\"network_config\"][\"reference_obs_size\"] :]],\n",
    "            axis=-1,\n",
    "        )\n",
    "        logits = jit_apply(decoder_param, concatenated)\n",
    "        action = jnp.array(action_distribution.mode(logits))\n",
    "        state = jit_env_step(state, action)\n",
    "        qposes.append(state.pipeline_state.qpos)\n",
    "\n",
    "ego_obs = np.array(ego_obs).T\n",
    "frames_mj = render_rollout(jnp.array(qposes))\n",
    "print(\"Rollout videos with 20 repeats of recorded intention from 130 to 230\")\n",
    "# display_video(frames_mj, framerate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dummy data. Replace this with your actual ego_obs.\n",
    "# ego_obs should be a numpy array of shape (147, 499) (features x time).\n",
    "ego_obs = np.array(ego_obs).T\n",
    "# Parameters\n",
    "window_size = 100\n",
    "total_time = ego_obs.shape[1]  # should be 499\n",
    "title = \"Online EgoObs (Feed into Decoder)\"\n",
    "# feature_mask = [75, 100]\n",
    "# Use multiprocessing to parallelize the rendering.\n",
    "f = functools.partial(visualize_ego_observation, ego_obs, window_size, title)\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    frames_plt = pool.map(f, list(range(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_video = np.concatenate((frames_mj, frames_plt), axis=2)\n",
    "display_video(combined_video, framerate=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "focus on gait related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dummy data. Replace this with your actual ego_obs.\n",
    "# ego_obs should be a numpy array of shape (147, 499) (features x time).\n",
    "# Parameters\n",
    "window_size = 100\n",
    "total_time = ego_obs.shape[1]  # should be 499\n",
    "title = \"Online EgoObs (Feed into Decoder)\"\n",
    "feature_mask = [75, 100]\n",
    "# Use multiprocessing to parallelize the rendering.\n",
    "f = functools.partial(\n",
    "    visualize_ego_observation, ego_obs, window_size, title, feature_mask=feature_mask\n",
    ")\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    frames_plt = pool.map(f, list(range(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_video = np.concatenate((frames_mj, frames_plt), axis=2)\n",
    "display_video(combined_video, framerate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc. Useful Utils -- Param Shape Comparison\n",
    "\n",
    "The following cell compares the parameter structure (the parameter here refers to the `flax.linen` model parameter, which is usually a `pytree` object) between two checkpoints. When reconstructing the model and aligning with the submodule we created, the following utilities are essential to ensure that the parameters in the checkpoint are identical and applicable to the created model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from track_mjx.agent.intention_network import Decoder\n",
    "\n",
    "\n",
    "def compare_param_structure(params1, params2, path=\"\"):\n",
    "    \"\"\"\n",
    "    Recursively compare the structure of two parameter dictionaries.\n",
    "    Returns a list of strings describing any key or shape differences.\n",
    "    \"\"\"\n",
    "    differences = []\n",
    "    # Compare keys in params1 against params2\n",
    "    for key in params1:\n",
    "        key_path = f\"{path}/{key}\" if path else key\n",
    "        if key not in params2:\n",
    "            differences.append(\n",
    "                f\"Key '{key_path}' exists in init params but not in checkpoint params.\"\n",
    "            )\n",
    "        else:\n",
    "            val1 = params1[key]\n",
    "            val2 = params2[key]\n",
    "            # If both values are dicts, compare recursively.\n",
    "            if isinstance(val1, dict) and isinstance(val2, dict):\n",
    "                differences.extend(compare_param_structure(val1, val2, key_path))\n",
    "            else:\n",
    "                # Compare shape if the object has a shape attribute (e.g., a JAX array).\n",
    "                shape1 = getattr(val1, \"shape\", None)\n",
    "                shape2 = getattr(val2, \"shape\", None)\n",
    "                if shape1 != shape2:\n",
    "                    differences.append(\n",
    "                        f\"Key '{key_path}' has shape {shape1} in init params vs {shape2} in checkpoint params.\"\n",
    "                    )\n",
    "    # Check for keys in params2 that are not in params1.\n",
    "    for key in params2:\n",
    "        key_path = f\"{path}/{key}\" if path else key\n",
    "        if key not in params1:\n",
    "            differences.append(\n",
    "                f\"Key '{key_path}' exists in checkpoint params but not in init params.\"\n",
    "            )\n",
    "    return differences\n",
    "\n",
    "\n",
    "# Assume your configuration dictionary is available as cfg.\n",
    "network_config = cfg[\"network_config\"]\n",
    "\n",
    "# Create the Decoder module.\n",
    "decoder = Decoder(\n",
    "    network_config[\"decoder_layer_sizes\"] + [network_config[\"action_size\"] * 2]\n",
    ")\n",
    "\n",
    "# IMPORTANT: For Dense layers, ensure to include a batch dimension.\n",
    "input_shape = (1, 60 + 147)\n",
    "dummy_input = jnp.zeros(input_shape)\n",
    "\n",
    "# Initialize the decoder's parameters.\n",
    "init_variables = decoder.init(jax.random.PRNGKey(0), dummy_input)\n",
    "params_init = init_variables[\"params\"]\n",
    "\n",
    "# Assume decoder_param holds your checkpoint parameters.\n",
    "# If checkpoint parameters are wrapped in a 'params' key, extract them.\n",
    "params_checkpoint = decoder_raw\n",
    "if \"params\" in params_checkpoint:\n",
    "    params_checkpoint = params_checkpoint[\"params\"]\n",
    "\n",
    "# Compare the structure of the two parameter dictionaries.\n",
    "structure_diffs = compare_param_structure(params_init, params_checkpoint)\n",
    "if structure_diffs:\n",
    "    print(\"Structure differences between init params and checkpoint params:\")\n",
    "    for diff in structure_diffs:\n",
    "        print(\" -\", diff)\n",
    "else:\n",
    "    print(\"No structural differences found between the parameter dictionaries.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track-mjx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
