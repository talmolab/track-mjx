{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "main_path = Path().resolve().parent\n",
    "if str(main_path) not in sys.path:\n",
    "    sys.path.append(str(main_path))\n",
    "    \n",
    "import orbax.checkpoint as ocp\n",
    "from track_mjx.agent.custom_ppo import TrainingState\n",
    "from track_mjx.agent import custom_losses as ppo_losses\n",
    "from track_mjx.agent import custom_ppo, custom_ppo_networks\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "from brax import envs\n",
    "from brax.training.acme import running_statistics, specs\n",
    "from track_mjx.environment import custom_wrappers\n",
    "\n",
    "from track_mjx.environment.task.multi_clip_tracking import MultiClipTracking\n",
    "from track_mjx.environment.task.single_clip_tracking import SingleClipTracking\n",
    "from track_mjx.environment.task.reward import RewardConfig\n",
    "\n",
    "from track_mjx.environment.walker.rodent import Rodent\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import OmegaConf\n",
    "import pickle\n",
    "import functools\n",
    "\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"./../track_mjx/config\")\n",
    "\n",
    "# Load the config file\n",
    "cfg = compose(config_name=\"rodent-full-clips\")\n",
    "\n",
    "# register the environment\n",
    "envs.register_environment(\"rodent_single_clip\", SingleClipTracking)\n",
    "envs.register_environment(\"rodent_multi_clip\", MultiClipTracking)\n",
    "envs.register_environment(\"fly_multi_clip\", MultiClipTracking)\n",
    "\n",
    "env_cfg = OmegaConf.to_container(cfg, resolve=True)\n",
    "env_args = cfg.env_config[\"env_args\"]\n",
    "env_rewards = cfg.env_config[\"reward_weights\"]\n",
    "train_config = cfg.train_setup[\"train_config\"]\n",
    "walker_config = cfg[\"walker_config\"]\n",
    "traj_config = cfg[\"reference_config\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `TrainingState`\n",
    "\n",
    "Currently I just directly copy and paste the scripts from the training code. In the future for the sake of runtime and cleaner code, we can have a helper function that construct the abstracted `TrainingState` directly without creating the actual environment. This is on Scott's TODOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self._steps_for_cur_frame: 2.0\n",
      "episode_length 390.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/vast/kaiwen/track-mjx/track_mjx/environment/task/single_clip_tracking.py:157: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  reference_frame = jax.tree_map(\n",
      "/root/vast/kaiwen/track-mjx/track_mjx/environment/task/multi_clip_tracking.py:134: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(lambda x: x[info[\"clip_idx\"]], self._reference_clips)\n",
      "/root/vast/kaiwen/track-mjx/track_mjx/environment/task/multi_clip_tracking.py:134: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(lambda x: x[info[\"clip_idx\"]], self._reference_clips)\n"
     ]
    }
   ],
   "source": [
    "# TODO(Scott): move this to track_mjx.io module\n",
    "input_data_path = \"/root/vast/scott-yang/track-mjx/data/ReferenceClip.p\"\n",
    "with open(input_data_path, \"rb\") as file:\n",
    "    reference_clip = pickle.load(file)\n",
    "\n",
    "walker_map = {\n",
    "    \"rodent\": Rodent,\n",
    "}\n",
    "\n",
    "walker_class = walker_map[env_cfg[\"walker_type\"]]\n",
    "walker = walker_class(**walker_config)\n",
    "\n",
    "# didn't use args** since penalty_pos_distance_scale need conversion\n",
    "reward_config = RewardConfig(\n",
    "    too_far_dist=env_rewards.too_far_dist,\n",
    "    bad_pose_dist=env_rewards.bad_pose_dist,\n",
    "    bad_quat_dist=env_rewards.bad_quat_dist,\n",
    "    ctrl_cost_weight=env_rewards.ctrl_cost_weight,\n",
    "    ctrl_diff_cost_weight=env_rewards.ctrl_diff_cost_weight,\n",
    "    pos_reward_weight=env_rewards.pos_reward_weight,\n",
    "    quat_reward_weight=env_rewards.quat_reward_weight,\n",
    "    joint_reward_weight=env_rewards.joint_reward_weight,\n",
    "    angvel_reward_weight=env_rewards.angvel_reward_weight,\n",
    "    bodypos_reward_weight=env_rewards.bodypos_reward_weight,\n",
    "    endeff_reward_weight=env_rewards.endeff_reward_weight,\n",
    "    healthy_z_range=env_rewards.healthy_z_range,\n",
    "    pos_reward_exp_scale=env_rewards.pos_reward_exp_scale,\n",
    "    quat_reward_exp_scale=env_rewards.quat_reward_exp_scale,\n",
    "    joint_reward_exp_scale=env_rewards.joint_reward_exp_scale,\n",
    "    angvel_reward_exp_scale=env_rewards.angvel_reward_exp_scale,\n",
    "    bodypos_reward_exp_scale=env_rewards.bodypos_reward_exp_scale,\n",
    "    endeff_reward_exp_scale=env_rewards.endeff_reward_exp_scale,\n",
    "    penalty_pos_distance_scale=jnp.array(env_rewards.penalty_pos_distance_scale),\n",
    ")\n",
    "\n",
    "# Automatically match dict keys and func needs\n",
    "environment = envs.get_environment(\n",
    "    env_name=cfg.env_config.env_name,\n",
    "    reference_clip=reference_clip,\n",
    "    walker=walker,\n",
    "    reward_config=reward_config,\n",
    "    **env_args,\n",
    "    **traj_config,\n",
    ")\n",
    "# Episode length is equal to (clip length - random init range - traj length) * steps per cur frame.\n",
    "# Will work on not hardcoding these values later\n",
    "episode_length = (\n",
    "    traj_config.clip_length - traj_config.random_init_range - traj_config.traj_length\n",
    ") * environment._steps_for_cur_frame\n",
    "print(f\"episode_length {episode_length}\")\n",
    "\n",
    "network_factory = functools.partial(\n",
    "    custom_ppo_networks.make_intention_ppo_networks,\n",
    "    encoder_hidden_layer_sizes=tuple(cfg.network_config.encoder_layer_sizes),\n",
    "    decoder_hidden_layer_sizes=tuple(cfg.network_config.decoder_layer_sizes),\n",
    "    value_hidden_layer_sizes=tuple(cfg.network_config.critic_layer_sizes),\n",
    ")\n",
    "\n",
    "seed = 42\n",
    "\n",
    "key = jax.random.PRNGKey(seed)\n",
    "global_key, local_key = jax.random.split(key)\n",
    "local_key, key_env, eval_key = jax.random.split(local_key, 3)\n",
    "# key_networks should be global, so that networks are initialized the same\n",
    "# way for different processes.\n",
    "key_policy, key_value, policy_params_fn_key = jax.random.split(global_key, 3)\n",
    "\n",
    "v_randomization_fn = None\n",
    "\n",
    "if isinstance(environment, envs.Env):\n",
    "    wrap_for_training = custom_wrappers.wrap\n",
    "else:\n",
    "    wrap_for_training = custom_wrappers.wrap\n",
    "\n",
    "train_env = wrap_for_training(\n",
    "    environment,\n",
    "    episode_length=episode_length,\n",
    "    action_repeat=1,\n",
    "    randomization_fn=v_randomization_fn,\n",
    ")\n",
    "\n",
    "reset_fn = train_env.reset\n",
    "key_envs = jax.random.split(key_env, 1)\n",
    "env_state = reset_fn(key_envs)\n",
    "\n",
    "normalize = lambda x, y: x\n",
    "if True:\n",
    "    normalize = running_statistics.normalize\n",
    "ppo_network = network_factory(\n",
    "    env_state.obs.shape[-1],\n",
    "    int(env_state.info[\"reference_obs_size\"][0]),\n",
    "    train_env.action_size,\n",
    "    preprocess_observations_fn=normalize,\n",
    ")\n",
    "make_policy = custom_ppo_networks.make_inference_fn(ppo_network)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=1e-4)\n",
    "\n",
    "init_params = ppo_losses.PPONetworkParams(\n",
    "    policy=ppo_network.policy_network.init(key_policy),\n",
    "    value=ppo_network.value_network.init(key_value),\n",
    ")\n",
    "\n",
    "training_state = TrainingState(  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
    "    optimizer_state=optimizer.init(init_params),  # pytype: disable=wrong-arg-types  # numpy-scalars\n",
    "    params=init_params,\n",
    "    normalizer_params=running_statistics.init_state(specs.Array(env_state.obs.shape[-1:], jnp.dtype(\"float32\"))),\n",
    "    env_steps=0,\n",
    ")\n",
    "\n",
    "abstract_policy = (training_state.normalizer_params, training_state.params.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orbax Checkpoint Related\n",
    "\n",
    "The following code will create the checkpoint manager and restore the checkpoint for both the policy and train state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest checkpoint step: 1053736960\n"
     ]
    }
   ],
   "source": [
    "# restore the whole model (both policy module, and training state)\n",
    "options = ocp.CheckpointManagerOptions(step_prefix=\"PPONetwork\")\n",
    "with ocp.CheckpointManager(\n",
    "    \"/root/vast/scott-yang/track-mjx/model_checkpoints/rodent_data/ReferenceClip.p_c52ed4\",\n",
    "    options=options,\n",
    ") as mngr:\n",
    "    print(f\"latest checkpoint step: {mngr.latest_step()}\")\n",
    "    policy = mngr.restore(\n",
    "        mngr.latest_step(), args=ocp.args.Composite(policy=ocp.args.StandardRestore(abstract_policy))\n",
    "    )[\"policy\"]\n",
    "    train_state = mngr.restore(\n",
    "        mngr.latest_step(), args=ocp.args.Composite(train_state=ocp.args.StandardRestore(training_state))\n",
    "    )[\"train_state\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering from the loaded checkpoint\n",
    "\n",
    "For this, I created a simplified version of the `policy_param_fn`, which just render the videos and save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "def display_video(frames, framerate=30):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        frames (array): (n_frames, height, width, 3)\n",
    "        framerate (int)\n",
    "    \"\"\"\n",
    "    height, width, _ = frames[0].shape\n",
    "    dpi = 70\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use(\"Agg\")  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "    plt.close(\"all\")  # Figure auto-closing upon backend switching is deprecated.\n",
    "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "\n",
    "    def update(frame):\n",
    "        im.set_data(frame)\n",
    "        return [im]\n",
    "\n",
    "    interval = 1000 / framerate\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames, interval=interval, blit=True, repeat=False)\n",
    "    return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/vast/kaiwen/track-mjx/track_mjx/environment/task/single_clip_tracking.py:157: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  reference_frame = jax.tree_map(\n",
      "/root/vast/kaiwen/track-mjx/track_mjx/environment/task/multi_clip_tracking.py:134: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(lambda x: x[info[\"clip_idx\"]], self._reference_clips)\n",
      "/root/vast/kaiwen/track-mjx/track_mjx/environment/task/multi_clip_tracking.py:134: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(lambda x: x[info[\"clip_idx\"]], self._reference_clips)\n",
      "E0120 02:30:56.002812  471435 pjrt_stream_executor_client.cc:3085] Execution of replica 0 failed: INTERNAL: Failed to allocate 169242000 bytes for new constant\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "INTERNAL: Failed to allocate 169242000 bytes for new constant",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrack_mjx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m render_rollout\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n\u001b[0;32m----> 4\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mrender_rollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmngr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m display_video(frames, framerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m~/vast/kaiwen/track-mjx/track_mjx/agent/logging.py:313\u001b[0m, in \u001b[0;36mrender_rollout\u001b[0;34m(num_steps, make_policy, params, rollout_key, cfg, env, model_path)\u001b[0m\n\u001b[1;32m    310\u001b[0m rollout_key, reset_rng, act_rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rollout_key, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# do a rollout on the saved model\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mjit_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreset_rng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m rollout \u001b[38;5;241m=\u001b[39m [state]\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(ref_trak_config\u001b[38;5;241m.\u001b[39mclip_length \u001b[38;5;241m*\u001b[39m env\u001b[38;5;241m.\u001b[39m_steps_for_cur_frame)):\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/track_mjx/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1287\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1285\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1287\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1290\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INTERNAL: Failed to allocate 169242000 bytes for new constant"
     ]
    }
   ],
   "source": [
    "from track_mjx.agent.logging import render_rollout\n",
    "from IPython.display import HTML\n",
    "\n",
    "frames = render_rollout(mngr.latest_step(), make_policy, policy, key_policy, cfg, environment, \".\")\n",
    "\n",
    "display_video(frames, framerate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track_mjx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
