{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JaX Flax --> ONNX\n",
    "\n",
    "This notebook converts brax MLP networks to an ONNX checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "import jax\n",
    "\n",
    "# Enable persistent compilation cache.\n",
    "jax.config.update(\"jax_compilation_cache_dir\", \"/tmp/jax_cache\")\n",
    "jax.config.update(\"jax_persistent_cache_min_entry_size_bytes\", -1)\n",
    "jax.config.update(\"jax_persistent_cache_min_compile_time_secs\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 06:05:57.684347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741759557.700928   36020 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741759557.706371   36020 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1741759557.719727   36020 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1741759557.719740   36020 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1741759557.719742   36020 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1741759557.719744   36020 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from mujoco_playground.config import locomotion_params, manipulation_params\n",
    "from mujoco_playground import locomotion, manipulation\n",
    "import functools\n",
    "import pickle\n",
    "import jax.numpy as jp\n",
    "import jax\n",
    "import tf2onnx\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import onnxruntime as rt\n",
    "from brax.training.acme import running_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from /root/vast/scott-yang/track-mjx/model_checkpoints/250306_194809 at step 144\n",
      "env._steps_for_cur_frame: 2.0\n"
     ]
    }
   ],
   "source": [
    "from track_mjx.environment.walker import rodent\n",
    "from track_mjx.agent import checkpointing\n",
    "from track_mjx.agent.intention_network import Decoder\n",
    "from track_mjx.analysis.rollout import create_environment\n",
    "from track_mjx.environment.wrappers import EvalClipResetWrapper\n",
    "from track_mjx.analysis.render import render_from_saved_rollout, display_video\n",
    "from track_mjx.analysis.utils import save_to_h5py, load_from_h5py\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from brax.training import distribution\n",
    "from brax.training.acme import running_statistics\n",
    "from pathlib import Path\n",
    "\n",
    "# replace with your checkpoint path\n",
    "ckpt_path = \"/root/vast/scott-yang/track-mjx/model_checkpoints/250306_194809\"\n",
    "# Load config from checkpoint\n",
    "ckpt = checkpointing.load_checkpoint_for_eval(ckpt_path)\n",
    "\n",
    "cfg = ckpt[\"cfg\"]\n",
    "\n",
    "# make some changes to the config\n",
    "# replace with absolute path to your data\n",
    "# -- your notebook may not have access to the same relative path\n",
    "cfg.data_path = \"/root/vast/scott-yang/track-mjx/data/transform_snips.h5\"\n",
    "cfg.train_setup.checkpoint_to_restore = ckpt_path\n",
    "\n",
    "# NOTE: To use accelerated JAX.JIT, only run the following code only once.\n",
    "# If you re-run, you will triggers recompilations\n",
    "\n",
    "env = create_environment(cfg)\n",
    "\n",
    "# env_name = \"BerkeleyHumanoidJoystickFlatTerrain\"\n",
    "# # ppo_params = locomotion_params.brax_ppo_config(env_name)\n",
    "# ppo_params = locomotion_params.brax_ppo_config(env_name)z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617 38\n"
     ]
    }
   ],
   "source": [
    "obs_size = env.observation_size\n",
    "act_size = env.action_size\n",
    "print(obs_size, act_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the abstract decoder\n",
    "network_config = cfg[\"network_config\"]\n",
    "\n",
    "# initialize the decoder with last layer represent the mean and variance of the action distribution\n",
    "decoder = Decoder(\n",
    "    network_config[\"decoder_layer_sizes\"] + [network_config[\"action_size\"] * 2]\n",
    ")\n",
    "\n",
    "normalizer = running_statistics.normalize\n",
    "# load the normalizer parameters\n",
    "normalizer_param = ckpt[\"policy\"][0]\n",
    "\n",
    "# load the decoder parameters\n",
    "decoder_raw = ckpt[\"policy\"][1][\"params\"][\"decoder\"]\n",
    "decoder_param = {\"params\": decoder_raw}\n",
    "# initialize the action distribution\n",
    "action_distribution = distribution.NormalTanhDistribution(\n",
    "    event_size=network_config[\"action_size\"]\n",
    ")\n",
    "\n",
    "# prevent recompilation\n",
    "jit_env_reset = jax.jit(env.reset, static_argnames=(\"clip_idx\",))\n",
    "jit_env_step = jax.jit(env.step)\n",
    "jit_apply = jax.jit(decoder.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class DecoderTF(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes,\n",
    "        activation=tf.nn.relu,\n",
    "        kernel_init=\"lecun_uniform\",\n",
    "        activate_final=False,\n",
    "        bias=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the Decoder.\n",
    "\n",
    "        Args:\n",
    "            layer_sizes (Sequence[int]): List of layer sizes for each Dense layer.\n",
    "            activation (callable): Activation function to apply.\n",
    "            kernel_init (str or callable): Kernel initializer for Dense layers.\n",
    "            activate_final (bool): Whether to apply activation (and layer norm) on the final layer.\n",
    "            bias (bool): Whether the Dense layers should use a bias term.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation = activation\n",
    "        self.kernel_init = kernel_init\n",
    "        self.activate_final = activate_final\n",
    "        self.bias = bias\n",
    "\n",
    "        # Build lists to store Dense layers and corresponding LayerNorm layers.\n",
    "        self.dense_layers = []\n",
    "        self.layer_norms = []\n",
    "\n",
    "        for i, size in enumerate(self.layer_sizes):\n",
    "            dense_layer = layers.Dense(\n",
    "                size,\n",
    "                kernel_initializer=self.kernel_init,\n",
    "                use_bias=self.bias,\n",
    "                name=f\"hidden_{i}\",\n",
    "            )\n",
    "            self.dense_layers.append(dense_layer)\n",
    "            # Apply activation and layer norm if it's not the final layer or if activate_final is True.\n",
    "            if i != len(self.layer_sizes) - 1 or self.activate_final:\n",
    "                self.layer_norms.append(\n",
    "                    layers.LayerNormalization(name=f\"LayerNorm_{i}\")\n",
    "                )\n",
    "            else:\n",
    "                self.layer_norms.append(None)\n",
    "\n",
    "    def call(self, inputs, get_activation=False):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input tensor.\n",
    "            get_activation (bool): If True, also return a dict with activations per layer.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor or Tuple[tf.Tensor, dict]: The output tensor, and optionally a dictionary\n",
    "            mapping layer names to their activations.\n",
    "        \"\"\"\n",
    "        activations = {}\n",
    "        x = inputs\n",
    "        for i, dense_layer in enumerate(self.dense_layers):\n",
    "            x = dense_layer(x)\n",
    "            # Apply activation (and layer norm) for all layers except the final one\n",
    "            # unless activate_final is True.\n",
    "            if i != len(self.layer_sizes) - 1 or self.activate_final:\n",
    "                x = self.activation(x)\n",
    "                if self.layer_norms[i] is not None:\n",
    "                    x = self.layer_norms[i](x)\n",
    "                if get_activation:\n",
    "                    activations[f\"layer_{i}\"] = x\n",
    "        if get_activation:\n",
    "            return x, activations\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_policy_network(\n",
    "    param_size,\n",
    "    hidden_layer_sizes=[512, 512, 512],\n",
    "):\n",
    "    policy_network = DecoderTF(\n",
    "        layer_sizes=list(hidden_layer_sizes) + [param_size],\n",
    "    )\n",
    "    return policy_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_policy_network = make_policy_network(param_size=act_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bias', 'scale'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_raw[\"LayerNorm_0\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 76)\n"
     ]
    }
   ],
   "source": [
    "example_input = tf.zeros((1, 60 + 147))\n",
    "example_output = tf_policy_network(example_input)\n",
    "print(example_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Dense name=hidden_0, built=True>,\n",
       " <LayerNormalization name=LayerNorm_0, built=True>,\n",
       " <Dense name=hidden_1, built=True>,\n",
       " <LayerNormalization name=LayerNorm_1, built=True>,\n",
       " <Dense name=hidden_2, built=True>,\n",
       " <LayerNormalization name=LayerNorm_2, built=True>,\n",
       " <Dense name=hidden_3, built=True>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_policy_network.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def transfer_weights(jax_params, tf_model):\n",
    "    \"\"\"\n",
    "    Transfer weights from a JAX parameter dictionary to the TensorFlow model.\n",
    "\n",
    "    Parameters:\n",
    "    - jax_params: dict\n",
    "      Nested dictionary with structure {block_name: {layer_name: {params}}}.\n",
    "      For example:\n",
    "      {\n",
    "        'CNN_0': {\n",
    "          'Conv_0': {'kernel': np.ndarray},\n",
    "          'Conv_1': {'kernel': np.ndarray},\n",
    "          'Conv_2': {'kernel': np.ndarray},\n",
    "        },\n",
    "        'MLP_0': {\n",
    "          'hidden_0': {'kernel': np.ndarray, 'bias': np.ndarray},\n",
    "          'hidden_1': {'kernel': np.ndarray, 'bias': np.ndarray},\n",
    "          'hidden_2': {'kernel': np.ndarray, 'bias': np.ndarray},\n",
    "        }\n",
    "      }\n",
    "\n",
    "    - tf_model: tf.keras.Model\n",
    "      An instance of the adapted VisionMLP model containing named submodules and layers.\n",
    "    \"\"\"\n",
    "    for layer_name, layer_params in jax_params.items():\n",
    "        try:\n",
    "            tf_layer = tf_model.get_layer(name=layer_name)\n",
    "        except ValueError:\n",
    "            print(f\"Layer {layer_name} not found in TensorFlow model.\")\n",
    "            continue\n",
    "        if isinstance(tf_layer, tf.keras.layers.Dense):\n",
    "            kernel = np.array(layer_params[\"kernel\"])\n",
    "            bias = np.array(layer_params[\"bias\"])\n",
    "            print(\n",
    "                f\"Transferring Dense layer {layer_name}, kernel shape {kernel.shape}, bias shape {bias.shape}\"\n",
    "            )\n",
    "            tf_layer.set_weights([kernel, bias])\n",
    "        elif isinstance(tf_layer, tf.keras.layers.LayerNormalization):\n",
    "            gamma = np.array(layer_params[\"scale\"])\n",
    "            beta = np.array(layer_params[\"bias\"])\n",
    "            print(\n",
    "                f\"Transferring LayerNorm layer {layer_name}, gamma shape {gamma.shape}, beta shape {beta.shape}\"\n",
    "            )\n",
    "            tf_layer.set_weights([gamma, beta])\n",
    "        else:\n",
    "            print(f\"Unhandled layer type in {layer_name}: {type(tf_layer)}\")\n",
    "\n",
    "    print(\"Weights transferred successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferring LayerNorm layer LayerNorm_0, gamma shape (512,), beta shape (512,)\n",
      "Transferring LayerNorm layer LayerNorm_1, gamma shape (512,), beta shape (512,)\n",
      "Transferring LayerNorm layer LayerNorm_2, gamma shape (512,), beta shape (512,)\n",
      "Transferring Dense layer hidden_0, kernel shape (207, 512), bias shape (512,)\n",
      "Transferring Dense layer hidden_1, kernel shape (512, 512), bias shape (512,)\n",
      "Transferring Dense layer hidden_2, kernel shape (512, 512), bias shape (512,)\n",
      "Transferring Dense layer hidden_3, kernel shape (512, 76), bias shape (76,)\n",
      "Weights transferred successfully.\n"
     ]
    }
   ],
   "source": [
    "transfer_weights(decoder_raw, tf_policy_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow prediction: [[ 1.8439487   1.8195764   0.96545035 -0.439171   -0.40265936  0.2564969\n",
      "   0.1916398   0.7053365   0.10381285 -0.8021101   1.0065378   0.4596085\n",
      "   0.63145524 -0.3105858   0.07498297 -0.46999425  0.20024359 -0.73598313\n",
      "  -1.059566   -0.02754471  0.0551969  -0.17984359 -0.02428782  0.24375072\n",
      "  -0.72107244 -0.9132743   0.18826191 -0.68193454 -0.06793734  0.08407827\n",
      "  -0.16330367  0.8781642  -0.5258842  -1.2544575  -0.27115586 -0.5708299\n",
      "  -0.6491313   0.3832001   2.5505161   2.4471612   3.5239034   1.0350533\n",
      "   1.0109748   0.5961331   0.20818818  1.012076    0.9032251   0.99604803\n",
      "   1.6650752   1.0874788   2.1373665   0.8951385   1.0131183   1.1382874\n",
      "   2.09338     2.1891723   2.6804817   0.3315918   0.00923654  0.72923476\n",
      "   0.29644024  1.1556864   0.81286275  1.0483586   0.6993547   0.7837227\n",
      "   0.21743718  0.48821545  0.28367886  0.42269164  0.6529687   0.412511\n",
      "   0.7705666   0.99292445  0.6567806   0.11907423]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1741761195.481004   36020 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741761195.481153   36020 single_machine.cc:374] Starting new session\n",
      "I0000 00:00:1741761195.487490   36020 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 42374 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "I0000 00:00:1741761195.882569   36020 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 42374 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "I0000 00:00:1741761196.066661   36020 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "I0000 00:00:1741761196.066847   36020 single_machine.cc:374] Starting new session\n",
      "I0000 00:00:1741761196.073196   36020 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 42374 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "ERROR:tf2onnx.tfonnx:rewriter <function rewrite_constant_fold at 0x7f93a5039ee0>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n"
     ]
    }
   ],
   "source": [
    "output_path = \"decoder.onnx\"\n",
    "\n",
    "# Example inputs for the model\n",
    "test_input = [np.ones((1, 60 + 147), dtype=np.float32)]\n",
    "\n",
    "# Define the TensorFlow input signature\n",
    "spec = [tf.TensorSpec(shape=(1, 60 + 147), dtype=tf.float32, name=\"obs\")]\n",
    "\n",
    "tensorflow_pred = tf_policy_network(test_input)[0]\n",
    "# Build the model by calling it with example data\n",
    "print(f\"Tensorflow prediction: {tensorflow_pred}\")\n",
    "\n",
    "tf_policy_network.output_names = [\"continuous_actions\"]\n",
    "\n",
    "# opset 11 matches isaac lab.\n",
    "model_proto, _ = tf2onnx.convert.from_keras(\n",
    "    tf_policy_network, input_signature=spec, opset=11, output_path=output_path\n",
    ")\n",
    "\n",
    "# Run inference with ONNX Runtime\n",
    "output_names = [\"continuous_actions\"]\n",
    "providers = [\"CPUExecutionProvider\"]\n",
    "m = rt.InferenceSession(output_path, providers=providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX prediction shape: (76,)\n",
      "ONNX prediction: [ 1.843951    1.8195753   0.965451   -0.43917096 -0.40265906  0.2564969\n",
      "  0.19163963  0.70533675  0.10381252 -0.8021095   1.0065386   0.45960838\n",
      "  0.6314552  -0.31058574  0.07498334 -0.4699943   0.20024568 -0.7359829\n",
      " -1.0595651  -0.02754396  0.05519673 -0.1798436  -0.02428795  0.2437514\n",
      " -0.7210727  -0.91327393  0.18826176 -0.6819357  -0.06793791  0.08407864\n",
      " -0.16330372  0.8781644  -0.5258841  -1.254457   -0.27115592 -0.57083106\n",
      " -0.64913124  0.38320005  2.5505152   2.4471622   3.5239048   1.0350527\n",
      "  1.0109746   0.5961333   0.20818841  1.0120753   0.9032248   0.9960484\n",
      "  1.6650743   1.0874784   2.137367    0.8951385   1.013118    1.1382874\n",
      "  2.0933805   2.1891718   2.6804807   0.33159184  0.00923641  0.7292344\n",
      "  0.29644024  1.1556861   0.8128632   1.0483584   0.6993551   0.78372234\n",
      "  0.21743718  0.4882155   0.28367874  0.42269155  0.652969    0.41251102\n",
      "  0.7705667   0.9929241   0.65678036  0.11907405]\n"
     ]
    }
   ],
   "source": [
    "onnx_input = {\"obs\": np.ones((1, 147 + 60), dtype=np.float32)}\n",
    "# Prepare inputs for ONNX Runtime\n",
    "onnx_pred = m.run(output_names, onnx_input)[0][0]\n",
    "\n",
    "\n",
    "print(\"ONNX prediction shape:\", onnx_pred.shape)\n",
    "print(\"ONNX prediction:\", onnx_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = {\n",
    "    \"state\": jp.ones(obs_size[\"state\"]),\n",
    "    \"privileged_state\": jp.zeros(obs_size[\"privileged_state\"]),\n",
    "}\n",
    "jax_pred, _ = inference_fn(test_input, jax.random.PRNGKey(0))\n",
    "print(jax_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(onnx_pred.shape)\n",
    "print(tensorflow_pred.shape)\n",
    "print(jax_pred.shape)\n",
    "plt.plot(onnx_pred, label=\"onnx\")\n",
    "plt.plot(tensorflow_pred, label=\"tensorflow\")\n",
    "plt.plot(jax_pred, label=\"jax\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_cfg = locomotion.get_default_config(env_name)\n",
    "# env = locomotion.load(env_name, config=env_cfg)\n",
    "# jit_reset = jax.jit(env.reset)\n",
    "# jit_step = jax.jit(env.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the policy.\n",
    "\n",
    "# # env_cfg = locomotion.get_default_config(env_name)\n",
    "# # env_cfg.init_from_crouch = 0.0\n",
    "# # env = locomotion.load(env_name, config=env_cfg)\n",
    "# # env_cfg = manipulation.get_default_config(env_name)\n",
    "# # env = manipulation.load(env_name, config=env_cfg)\n",
    "# # jit_reset = jax.jit(env.reset)\n",
    "# # jit_step = jax.jit(env.step)\n",
    "\n",
    "# x = 0.8\n",
    "# y = 0.0\n",
    "# yaw = 0.3\n",
    "# command = jp.array([x, y, yaw])\n",
    "# # actions = []\n",
    "\n",
    "# states = [state := jit_reset(jax.random.PRNGKey(555))]\n",
    "# state.info[\"command\"] = command\n",
    "# for _ in range(env_cfg.episode_length):\n",
    "#   onnx_input = {'obs': np.array(state.obs[\"state\"].reshape(1, -1))}\n",
    "#   action = m.run(output_names, onnx_input)[0][0]\n",
    "#   state = jit_step(state, jp.array(action))\n",
    "#   state.info[\"command\"] = command\n",
    "#   states.append(state)\n",
    "#   # actions.append(state.info[\"motor_targets\"])\n",
    "#   # actions.append(action)\n",
    "#   if state.done:\n",
    "#     print(\"Unexpected termination.\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mediapy as media\n",
    "# fps = 1.0 / env.dt\n",
    "\n",
    "# frames = env.render(\n",
    "#     states,\n",
    "#     camera=\"track\",\n",
    "#     width=640*2,\n",
    "#     height=480*2,\n",
    "# )\n",
    "# media.show_video(frames, fps=fps, loop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track-mjx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
